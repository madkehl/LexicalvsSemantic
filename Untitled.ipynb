{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import ConcordanceIndex\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "vocab = nlp.vocab.strings\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "from re import sub\n",
    "punctuation = punctuation +'”'+'“'+'’' + '—' + '’' + '‘'\n",
    "\n",
    "\n",
    "test_doc = open(\"./test_doc.txt\", \"r\", encoding = \"utf-8\")\n",
    "test_doc = test_doc.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['whom','hast','thou','therein', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, topn=10):\n",
    "    #https://stackoverflow.com/questions/57697374/list-most-similar-words-in-spacy-in-pretrained-model\n",
    "    ms = nlp.vocab.vectors.most_similar(nlp(word).vector.reshape(1,nlp(word).vector.shape[0]), n=topn)\n",
    "    words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "    distances = ms[2]\n",
    "    return words, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def latent_meaning_spacy(i, top_ = 10):\n",
    "    '''\n",
    "    INPUT: word tuple, topn\n",
    "    \n",
    "    OUTPUT: word tuple, the distance between the two original words, and the distance between the topn related words\n",
    "    '''\n",
    "    if(i[0] in vocab) & (i[1] in vocab):\n",
    "        \n",
    "        first_close, first_close_distances = most_similar(i[0], topn= top_)\n",
    "        second_close, second_close_distances = most_similar(i[1], topn= top_)\n",
    "        first_vec = nlp.vocab[i[0]].vector\n",
    "        second_vec = nlp.vocab[i[1]].vector\n",
    "        item_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + nlp.vocab[z].vector\n",
    "        \n",
    "        for z in second_close:\n",
    "            second_vec = second_vec +  nlp.vocab[z].vector\n",
    "        \n",
    "        first_vec = first_vec - nlp.vocab[i[0]].vector\n",
    "        second_vec = second_vec - nlp.vocab[i[1]].vector\n",
    "        \n",
    "        latent_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, latent_dis])\n",
    "    else:\n",
    "        return([None, None, None])\n",
    "\n",
    "\n",
    "def latent_meaning(i, model3):\n",
    "    \n",
    "    if(i[0] in model3.wv.vocab) & (i[1] in model3.wv.vocab):\n",
    "        first_close = list(model3.wv.most_similar(i[0], topn= 5))\n",
    "        second_close = list(model3.wv.most_similar(i[1], topn= 5))\n",
    "        \n",
    "        first_vec = model3.wv.get_vector(i[0])\n",
    "        second_vec = model3.wv.get_vector(i[1])\n",
    "        \n",
    "        item_dis = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + model3.wv.get_vector(z[0])\n",
    "        for n in second_close:\n",
    "            second_vec = second_vec + model3.wv.get_vector(n[0])\n",
    "        \n",
    "        first_vec = first_vec - model3[i[0]]\n",
    "        second_vec = second_vec - model3[i[1]]\n",
    "        cos_sim = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, cos_sim])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PMI is defined as //pmi(r,c)=log(P(r,c)/(P(r)*P(c)))//, with P(r,c) being the\n",
    "#probability of co-occurrence and P(r) and P(c) the probability of\n",
    "#occurrence of two words (estimated via frequency)\n",
    "\n",
    "#- I considered words as co-occurring if they occurred within a window\n",
    "#of 5 words:\n",
    "#no no yes yes yes yes target yes yes yes yes no no\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ci(komyagin):\n",
    "    txt = (nltk.Text(komyagin))\n",
    "    return(ConcordanceIndex(txt))\n",
    "\n",
    "#for some reason having difficulty subsetting by tokens instead of characters:: instead just use enough \n",
    "#characters to later be able to reliably get a 5 word radius\n",
    "\n",
    "def get_context(ci, word, width=150, lines=100):\n",
    "    \n",
    "    half_width =  (width - len(word) - 2) // 2\n",
    "    context = width // 4 # approx number of words of context\n",
    "    num = 5\n",
    "    results = []\n",
    "    offsets = ci.offsets(word)\n",
    "    \n",
    "    if offsets:\n",
    "        for i in offsets:\n",
    "            query_word = ci._tokens[i]\n",
    "  \n",
    "            left_context = ci._tokens[max(0, i - context) : i]\n",
    "            right_context = ci._tokens[i + 1 : i + context]\n",
    "           \n",
    "            left_print = \" \".join(left_context)[-half_width:]\n",
    "            right_print = \" \".join(right_context)[:half_width]\n",
    "                \n",
    "            full_line_print = \" \".join([left_print, query_word, right_print])\n",
    "            \n",
    "            results.append(full_line_print)\n",
    "            \n",
    "    return ([num, results])\n",
    "\n",
    "def clean_text(txt_ls):\n",
    "    \n",
    "    translator = str.maketrans('','', sub('\\#', '', punctuation))\n",
    "\n",
    "    clean_txt_ls = []\n",
    "    for i in txt_ls:\n",
    "        n = i.split()\n",
    "        str_ = \"\"\n",
    "        for z in n:\n",
    "            z = z.lower()\n",
    "            s = z.translate(str.maketrans(translator))\n",
    "            if s not in stopwords:\n",
    "          #  print(s)\n",
    "                str_ = str_ + \" \" + s\n",
    "        clean_txt_ls.append(str_[1:])\n",
    "        \n",
    "    return(clean_txt_ls)\n",
    "\n",
    "def clean_context(ci, target_word1, target_word2, window = 5):\n",
    "    tox = []\n",
    "    #gets context around a target word\n",
    "    word_one_context = get_context(ci, target_word1)\n",
    "   \n",
    "     #takes this context and reshapes it into a -5 to + 5 window\n",
    "    for i in word_one_context[1]:\n",
    "        split_i = i.split()\n",
    "        for z in split_i:\n",
    "            tox.append(z)\n",
    "            \n",
    "    to_mend = list(enumerate(tox))\n",
    "    \n",
    "    mended_tox = []\n",
    "    #modifications necessary to account for words at the start and end of corpus\n",
    "    for z in to_mend:\n",
    "        if z[1] == target_word1:\n",
    "            if z[0] < (window):\n",
    "                padded = tox[0:(z[0]+window)]\n",
    "                mended_tox.append(padded)\n",
    "            elif z[0] > (len(tox) - 1):\n",
    "                padded = tox[(z[0] -window):(len(tox)-1)]\n",
    "                mended_tox.append(padded)\n",
    "            else: \n",
    "                padded = tox[(z[0] -window):(z[0] + window)]\n",
    "                mended_tox.append(padded)\n",
    "#reshaping list into format that can be more quickly processed    \n",
    "    final_tox = []\n",
    "    for i in mended_tox:\n",
    "        for n in i:\n",
    "            final_tox.append(n)\n",
    "    return(final_tox)\n",
    "\n",
    "\n",
    "#gathers all contexts of the word in results\n",
    "def mutual_informativity(ci, target_word1, target_word2, total_count, window = 10):\n",
    "   \n",
    "    final_tox =  clean_context(ci, target_word1, target_word2, window = 10)\n",
    "    \n",
    "#this will return the count of target_word2 in the vicinity of target_word1\n",
    "    prob_numx = Counter(final_tox)\n",
    "    prob_num = prob_numx[target_word2]\n",
    "    P_rc = prob_num/total_count\n",
    "    P_r = (count(ci, target_word1))/total_count\n",
    "    P_c = (count(ci, target_word2))/total_count\n",
    " #checking for indexing errors and overlap errors.\n",
    "#fix to overlap is a bit of a hack fix, most notable problems are double counting \n",
    "#so if number of cooccurances is greater than number of times a word appears in \n",
    "#a document, then it replaces cooccurance with the total number of times\n",
    "#except in the case that this is an odd number (since it's not being double-counted then,\n",
    "#it's being double counted once and has another appearance)\n",
    "#odd numbers, is P_c - 1/total. \n",
    "#this still may cause some errors\n",
    "    if P_rc == 0:\n",
    "        print(prob_numx)\n",
    "        return ([target_word1, target_word2, \"ERROR\"])\n",
    "    elif P_rc > P_c:\n",
    "        print(target_word2)\n",
    "        rounded = int(P_rc/P_c)\n",
    "        nr = P_rc/P_c\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c - 1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    elif P_rc > P_r:\n",
    "        rounded = int(P_rc/P_r)\n",
    "        nr = P_rc/P_r\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c -1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    else:\n",
    "        mutinf = math.log10(P_rc/(P_r*P_c))\n",
    "    return ([target_word1, target_word2, mutinf])\n",
    "\n",
    "def count(ci, word):\n",
    "   \n",
    "    offsets = ci.offsets(word)\n",
    "\n",
    "    return len(offsets)\n",
    "\n",
    "def pmi(text):\n",
    "    '''\n",
    "    iterates through and finds shit\n",
    "    '''\n",
    "    \n",
    "    clean_doc = clean_text(text.split())\n",
    "    total_count = len(clean_doc)\n",
    "    \n",
    "    test_ci = make_ci(clean_doc)\n",
    "    \n",
    "    pmi_list = []\n",
    "    ordered_set_hold = []\n",
    "    ordered_set = [i for i in clean_doc if i not in ordered_set_hold and len(i) > 0]\n",
    "\n",
    "    \n",
    "    index = 0\n",
    "    #realistically only words that at some point occur in a 3 word window are really worth looking at esp with these short texts\n",
    "    for i in enumerate(ordered_set):\n",
    "        if i[0] < (len((ordered_set))-1):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 1], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        if i[0] < (len(ordered_set)-2):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 2], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)        \n",
    "        if i[0] < (len(ordered_set)-3):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 3], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        else:\n",
    "            return(pmi_list)\n",
    "        \n",
    "#just to allow for sorting by actual pmi index\n",
    "def takeSecond(elem):\n",
    "    #print(elem[2])\n",
    "    return elem[2]\n",
    "\n",
    "def pmi_high(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = True)\n",
    "    return cat[:n]\n",
    "\n",
    "def pmi_low(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = False)\n",
    "    return cat[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madke\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  del sys.path[0]\n",
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "word_pairs = []\n",
    "item_dis = []\n",
    "latent_dis = []\n",
    "bar.start()\n",
    "x = 0\n",
    "for i in pmi_high(pmi_test, 500):\n",
    "    if i != None:\n",
    "        j, k , l = latent_meaning_spacy([i[0], i[1]])\n",
    "        word_pairs.append(j)\n",
    "        item_dis.append(k)\n",
    "        latent_dis.append(l)\n",
    "        bar.update(x + 1)\n",
    "        x = x + 1\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_meanings = pd.DataFrame({\n",
    "    \n",
    "    'word_pairs': word_pairs,\n",
    "    'item_dis': item_dis,\n",
    "    'latent_dis': latent_dis\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_meanings['difference']= latent_meanings['item_dis'] - latent_meanings['latent_dis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pairs</th>\n",
       "      <th>item_dis</th>\n",
       "      <th>latent_dis</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[pliny, proceeded]</td>\n",
       "      <td>-0.010289</td>\n",
       "      <td>-0.040020</td>\n",
       "      <td>0.029731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[punish, serpent]</td>\n",
       "      <td>0.158319</td>\n",
       "      <td>0.133861</td>\n",
       "      <td>0.024459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[isaiah, soever]</td>\n",
       "      <td>0.197830</td>\n",
       "      <td>0.187443</td>\n",
       "      <td>0.010387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word_pairs  item_dis  latent_dis  difference\n",
       "51  [pliny, proceeded] -0.010289   -0.040020    0.029731\n",
       "14   [punish, serpent]  0.158319    0.133861    0.024459\n",
       "23    [isaiah, soever]  0.197830    0.187443    0.010387"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_meanings[latent_meanings['difference'] > 0].sort_values(by = 'difference', ascending = False)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
