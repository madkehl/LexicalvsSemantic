{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import ConcordanceIndex\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "vocab = nlp.vocab.strings\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "from re import sub\n",
    "punctuation = punctuation +'”'+'“'+'’' + '—' + '’' + '‘'\n",
    "\n",
    "\n",
    "test_doc = open(\"./test_doc.txt\", \"r\", encoding = \"utf-8\")\n",
    "test_doc = test_doc.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['whom','hast','thou','therein', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, topn=10):\n",
    "    #https://stackoverflow.com/questions/57697374/list-most-similar-words-in-spacy-in-pretrained-model\n",
    "    ms = nlp.vocab.vectors.most_similar(nlp(word).vector.reshape(1,nlp(word).vector.shape[0]), n=topn)\n",
    "    words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "    distances = ms[2]\n",
    "    return words, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def latent_meaning_spacy(i, top_ = 10):\n",
    "    '''\n",
    "    INPUT: word tuple, topn\n",
    "    \n",
    "    OUTPUT: word tuple, the distance between the two original words, and the distance between the topn related words\n",
    "    '''\n",
    "    if(i[0] in vocab) & (i[1] in vocab):\n",
    "        \n",
    "        first_close, first_close_distances = most_similar(i[0], topn= top_)\n",
    "        second_close, second_close_distances = most_similar(i[1], topn= top_)\n",
    "        first_vec = nlp.vocab[i[0]].vector\n",
    "        second_vec = nlp.vocab[i[1]].vector\n",
    "        item_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + nlp.vocab[z].vector\n",
    "        \n",
    "        for z in second_close:\n",
    "            second_vec = second_vec +  nlp.vocab[z].vector\n",
    "        \n",
    "        first_vec = first_vec - nlp.vocab[i[0]].vector\n",
    "        second_vec = second_vec - nlp.vocab[i[1]].vector\n",
    "        \n",
    "        latent_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, latent_dis])\n",
    "\n",
    "\n",
    "def latent_meaning(i, model3):\n",
    "    \n",
    "    if(i[0] in model3.wv.vocab) & (i[1] in model3.wv.vocab):\n",
    "        first_close = list(model3.wv.most_similar(i[0], topn= 5))\n",
    "        second_close = list(model3.wv.most_similar(i[1], topn= 5))\n",
    "        \n",
    "        first_vec = model3.wv.get_vector(i[0])\n",
    "        second_vec = model3.wv.get_vector(i[1])\n",
    "        \n",
    "        item_dis = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + model3.wv.get_vector(z[0])\n",
    "        for n in second_close:\n",
    "            second_vec = second_vec + model3.wv.get_vector(n[0])\n",
    "        \n",
    "        first_vec = first_vec - model3[i[0]]\n",
    "        second_vec = second_vec - model3[i[1]]\n",
    "        cos_sim = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, cos_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PMI is defined as //pmi(r,c)=log(P(r,c)/(P(r)*P(c)))//, with P(r,c) being the\n",
    "#probability of co-occurrence and P(r) and P(c) the probability of\n",
    "#occurrence of two words (estimated via frequency)\n",
    "\n",
    "#- I considered words as co-occurring if they occurred within a window\n",
    "#of 5 words:\n",
    "#no no yes yes yes yes target yes yes yes yes no no\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ci(komyagin):\n",
    "    txt = (nltk.Text(komyagin))\n",
    "    return(ConcordanceIndex(txt))\n",
    "\n",
    "#for some reason having difficulty subsetting by tokens instead of characters:: instead just use enough \n",
    "#characters to later be able to reliably get a 5 word radius\n",
    "\n",
    "def get_context(ci, word, width=150, lines=100):\n",
    "    \n",
    "    half_width =  (width - len(word) - 2) // 2\n",
    "    context = width // 4 # approx number of words of context\n",
    "    num = 5\n",
    "    results = []\n",
    "    offsets = ci.offsets(word)\n",
    "    \n",
    "    if offsets:\n",
    "        for i in offsets:\n",
    "            query_word = ci._tokens[i]\n",
    "  \n",
    "            left_context = ci._tokens[max(0, i - context) : i]\n",
    "            right_context = ci._tokens[i + 1 : i + context]\n",
    "           \n",
    "            left_print = \" \".join(left_context)[-half_width:]\n",
    "            right_print = \" \".join(right_context)[:half_width]\n",
    "                \n",
    "            full_line_print = \" \".join([left_print, query_word, right_print])\n",
    "            \n",
    "            results.append(full_line_print)\n",
    "            \n",
    "    return ([num, results])\n",
    "\n",
    "def clean_text(txt_ls):\n",
    "    \n",
    "    translator = str.maketrans('','', sub('\\#', '', punctuation))\n",
    "\n",
    "    clean_txt_ls = []\n",
    "    for i in txt_ls:\n",
    "        n = i.split()\n",
    "        str_ = \"\"\n",
    "        for z in n:\n",
    "            z = z.lower()\n",
    "            s = z.translate(str.maketrans(translator))\n",
    "            if s not in stopwords:\n",
    "          #  print(s)\n",
    "                str_ = str_ + \" \" + s\n",
    "        clean_txt_ls.append(str_[1:])\n",
    "        \n",
    "    return(clean_txt_ls)\n",
    "\n",
    "def clean_context(ci, target_word1, target_word2, window = 5):\n",
    "    tox = []\n",
    "    #gets context around a target word\n",
    "    word_one_context = get_context(ci, target_word1)\n",
    "   \n",
    "     #takes this context and reshapes it into a -5 to + 5 window\n",
    "    for i in word_one_context[1]:\n",
    "        split_i = i.split()\n",
    "        for z in split_i:\n",
    "            tox.append(z)\n",
    "            \n",
    "    to_mend = list(enumerate(tox))\n",
    "    \n",
    "    mended_tox = []\n",
    "    #modifications necessary to account for words at the start and end of corpus\n",
    "    for z in to_mend:\n",
    "        if z[1] == target_word1:\n",
    "            if z[0] < (window):\n",
    "                padded = tox[0:(z[0]+window)]\n",
    "                mended_tox.append(padded)\n",
    "            elif z[0] > (len(tox) - 1):\n",
    "                padded = tox[(z[0] -window):(len(tox)-1)]\n",
    "                mended_tox.append(padded)\n",
    "            else: \n",
    "                padded = tox[(z[0] -window):(z[0] + window)]\n",
    "                mended_tox.append(padded)\n",
    "#reshaping list into format that can be more quickly processed    \n",
    "    final_tox = []\n",
    "    for i in mended_tox:\n",
    "        for n in i:\n",
    "            final_tox.append(n)\n",
    "    return(final_tox)\n",
    "\n",
    "\n",
    "#gathers all contexts of the word in results\n",
    "def mutual_informativity(ci, target_word1, target_word2, total_count, window = 10):\n",
    "   \n",
    "    final_tox =  clean_context(ci, target_word1, target_word2, window = 10)\n",
    "    \n",
    "#this will return the count of target_word2 in the vicinity of target_word1\n",
    "    prob_numx = Counter(final_tox)\n",
    "    prob_num = prob_numx[target_word2]\n",
    "    P_rc = prob_num/total_count\n",
    "    P_r = (count(ci, target_word1))/total_count\n",
    "    P_c = (count(ci, target_word2))/total_count\n",
    " #checking for indexing errors and overlap errors.\n",
    "#fix to overlap is a bit of a hack fix, most notable problems are double counting \n",
    "#so if number of cooccurances is greater than number of times a word appears in \n",
    "#a document, then it replaces cooccurance with the total number of times\n",
    "#except in the case that this is an odd number (since it's not being double-counted then,\n",
    "#it's being double counted once and has another appearance)\n",
    "#odd numbers, is P_c - 1/total. \n",
    "#this still may cause some errors\n",
    "    if P_rc == 0:\n",
    "        print(prob_numx)\n",
    "        return ([target_word1, target_word2, \"ERROR\"])\n",
    "    elif P_rc > P_c:\n",
    "        print(target_word2)\n",
    "        rounded = int(P_rc/P_c)\n",
    "        nr = P_rc/P_c\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c - 1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    elif P_rc > P_r:\n",
    "        rounded = int(P_rc/P_r)\n",
    "        nr = P_rc/P_r\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c -1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    else:\n",
    "        mutinf = math.log10(P_rc/(P_r*P_c))\n",
    "    return ([target_word1, target_word2, mutinf])\n",
    "\n",
    "def count(ci, word):\n",
    "   \n",
    "    offsets = ci.offsets(word)\n",
    "\n",
    "    return len(offsets)\n",
    "\n",
    "def pmi(text):\n",
    "    '''\n",
    "    iterates through and finds shit\n",
    "    '''\n",
    "    \n",
    "    clean_doc = clean_text(text.split())\n",
    "    total_count = len(clean_doc)\n",
    "    \n",
    "    test_ci = make_ci(clean_doc)\n",
    "    \n",
    "    pmi_list = []\n",
    "    ordered_set_hold = []\n",
    "    ordered_set = [i for i in clean_doc if i not in ordered_set_hold and len(i) > 0]\n",
    "\n",
    "    \n",
    "    index = 0\n",
    "    #realistically only words that at some point occur in a 3 word window are really worth looking at esp with these short texts\n",
    "    for i in enumerate(ordered_set):\n",
    "        if i[0] < (len((ordered_set))-1):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 1], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        if i[0] < (len(ordered_set)-2):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 2], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)        \n",
    "        if i[0] < (len(ordered_set)-3):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 3], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        else:\n",
    "            return(pmi_list)\n",
    "#just to allow for sorting by actual pmi index\n",
    "def takeSecond(elem):\n",
    "    #print(elem[2])\n",
    "    return elem[2]\n",
    "def strOnly(elem):\n",
    "   # print(elem[0])\n",
    "    \n",
    "    return elem[0]\n",
    "\n",
    "def strOnly2(elem):\n",
    "   # print(elem[0][0])\n",
    "    \n",
    "    return elem[0][0]\n",
    "\n",
    "def pmi_clean(text):\n",
    "    n = pmi(text)\n",
    "    sort1 = []\n",
    "    for i in n:\n",
    "        y = i[:2]\n",
    "        y.sort(key = strOnly)\n",
    "        x = [y[0], y[1], i[2]]\n",
    "        sort1.append(x)\n",
    "    q = sort1\n",
    "    #print(q)\n",
    "    q.sort(key = strOnly2)\n",
    "   # print(q)\n",
    "    k = list(enumerate(q))\n",
    "    pmi_cl = []\n",
    "    dumb_check = []\n",
    "    for z in k:\n",
    "        n = [z[1][0], z[1][1]]\n",
    "        if ((z[0]) < (len(k) -1)) and (n not in dumb_check):\n",
    "            #if((z[1][0] == k[z[0] + 1][1][0]) and (z[1][1] == k[z[0] + 1][1][1])) or ((z[1][0] == k[z[0] + 1][1][1]) and (z[1][1] == k[z[0] + 1][1][0])):\n",
    "            dumb_check.append(n)\n",
    "            #pmi_cl.append(k[z[0] + 1][1])\n",
    "            #elif (z[1] not in pmi_cl) and (n not in dumb_check):\n",
    "             #   dumb_check.append(n)\n",
    "            #  pmi_cl.append(z[1])\n",
    "        #elif (z[1] not in pmi_cl) and (n not in dumb_check):\n",
    "         #   dumb_check.append(n)\n",
    "            pmi_cl.append(z[1])\n",
    "    return pmi_cl\n",
    "            \n",
    "\n",
    "def pmi_high(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = True)\n",
    "    return cat[:n]\n",
    "\n",
    "def pmi_low(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = False)\n",
    "    return cat[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jonah\n",
      "jonah\n",
      "go\n",
      "ships\n",
      "go\n",
      "ships\n",
      "punish\n",
      "piercing\n",
      "piercing\n",
      "serpent\n",
      "even\n",
      "even\n",
      "crooked\n",
      "crooked\n",
      "serpent\n",
      "shall\n",
      "shall\n",
      "slay\n",
      "dragon\n",
      "beast\n",
      "sunrise\n",
      "many\n",
      "appeared\n",
      "among\n",
      "former\n",
      "us\n",
      "fly\n",
      "let\n",
      "fly\n",
      "let\n",
      "us\n",
      "let\n",
      "us\n",
      "fly\n",
      "us\n",
      "fly\n",
      "old\n",
      "fly\n",
      "old\n",
      "nick\n",
      "old\n",
      "nick\n",
      "ibid\n",
      "history\n",
      "ceti\n",
      "sperma\n",
      "ceti\n",
      "sperma\n",
      "ceti\n",
      "ceti\n",
      "vide\n",
      "vide\n",
      "v\n",
      "e\n",
      "statein\n",
      "works\n",
      "created\n",
      "swim\n",
      "stream\n",
      "hugest\n",
      "living\n",
      "creatures\n",
      "ibid\n",
      "mighty\n",
      "water\n",
      "sea\n",
      "oil\n",
      "oil\n",
      "swimming\n",
      "fullers\n",
      "ship\n",
      "boat\n",
      "upon\n",
      "schoutens\n",
      "sixth\n",
      "open\n",
      "whether\n",
      "see\n",
      "discoverer\n",
      "first\n",
      "discoverer\n",
      "ducat\n",
      "taken\n",
      "near\n",
      "shetland\n",
      "gods\n",
      "voice\n",
      "animal\n",
      "write\n",
      "asiatics\n",
      "killed\n",
      "towing\n",
      "either\n",
      "around\n",
      "vault\n",
      "heaven\n",
      "compare\n",
      "air\n",
      "small\n",
      "sized\n",
      "larger\n",
      "bore\n",
      "main\n",
      "waterworks\n",
      "london\n",
      "bridge\n",
      "inferior\n",
      "impetus\n",
      "animal\n",
      "till\n",
      "paean\n",
      "io\n",
      "sing\n",
      "sing\n",
      "finny\n",
      "peoples\n",
      "fatter\n",
      "pacific\n",
      "ocean\n",
      "less\n",
      "answered\n",
      "narrative\n",
      "shipwreck\n",
      "whale\n",
      "ship\n",
      "essex\n",
      "nantucket\n",
      "finally\n",
      "pacific\n",
      "ocean\n",
      "amounted\n",
      "altogether\n",
      "10440\n",
      "wide\n",
      "expanded\n",
      "jaws\n",
      "1839\n",
      "cachalot\n",
      "1839\n",
      "cachalot\n",
      "sperm\n",
      "better\n",
      "armed\n",
      "better\n",
      "armed\n",
      "true\n",
      "greenland\n",
      "right\n",
      "whale\n",
      "possessing\n",
      "formidable\n",
      "weapon\n",
      "raise\n",
      "wheel\n",
      "steady\n",
      "steady\n",
      "sir\n",
      "masthead\n",
      "sir\n",
      "masthead\n",
      "ahoy\n",
      "masthead\n",
      "ahoy\n",
      "see\n",
      "sir\n",
      "ay\n",
      "sir\n",
      "shoal\n",
      "sir\n",
      "shoal\n",
      "sperm\n",
      "shoal\n",
      "sperm\n",
      "breaches\n",
      "sing\n",
      "sing\n",
      "sing\n",
      "time\n",
      "time\n",
      "ay\n",
      "ay\n",
      "sir\n",
      "blows\n",
      "sir\n",
      "blows\n",
      "theretherethar\n",
      "blows\n",
      "theretherethar\n",
      "eight\n",
      "fell\n",
      "directly\n",
      "probably\n",
      "captors\n",
      "whalemans\n",
      "adventures\n",
      "send\n",
      "hell\n",
      "life\n",
      "comstock\n",
      "mutineer\n",
      "brother\n",
      "mutineer\n",
      "brother\n",
      "william\n",
      "another\n",
      "version\n",
      "whaleship\n",
      "ex\n",
      "pedestrians\n",
      "vicinity\n",
      "pedestrians\n",
      "vicinity\n",
      "london\n",
      "voyager\n",
      "ocean\n",
      "saw\n",
      "suddenly\n",
      "mighty\n",
      "miriam\n",
      "coffin\n",
      "whale\n",
      "fisherman\n",
      "whale\n",
      "harpooned\n",
      "harpooned\n",
      "sure\n",
      "bethink\n",
      "distended\n",
      "jaws\n",
      "boat\n",
      "threatening\n",
      "cheery\n",
      "lads\n",
      "nantucket\n",
      "song\n",
      "oh\n",
      "mid\n",
      "storm\n",
      "gale\n",
      "might\n",
      "boundless\n"
     ]
    }
   ],
   "source": [
    "pmi_test = pmi(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = []\n",
    "item_dis = []\n",
    "latent_dis = []\n",
    "for i in pmi_high(pmi_test, 20):\n",
    "    j, k , l = latent_meaning_spacy([i[0], i[1]])\n",
    "    word_pairs.append(j)\n",
    "    item_dis.append(k)\n",
    "    latent_dis.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_meanings = pd.DataFrame({\n",
    "    \n",
    "    'word_pairs': word_pairs,\n",
    "    'item_dis': item_dis,\n",
    "    'latent_dis': latent_dis,\n",
    "    'difference': item_dis - latent_dis\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pairs</th>\n",
       "      <th>item_dis</th>\n",
       "      <th>latent_dis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[genesis, path]</td>\n",
       "      <td>0.146657</td>\n",
       "      <td>0.321490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[path, shine]</td>\n",
       "      <td>0.172554</td>\n",
       "      <td>0.342823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[shine, think]</td>\n",
       "      <td>0.254291</td>\n",
       "      <td>0.338092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[think, hoary]</td>\n",
       "      <td>0.180608</td>\n",
       "      <td>0.329136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[hoary, prepared]</td>\n",
       "      <td>0.033687</td>\n",
       "      <td>0.208438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[day, sore]</td>\n",
       "      <td>0.340400</td>\n",
       "      <td>0.352394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[sore, strong]</td>\n",
       "      <td>0.298166</td>\n",
       "      <td>0.409027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[sore, sword]</td>\n",
       "      <td>0.190398</td>\n",
       "      <td>0.215757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[strong, sword]</td>\n",
       "      <td>0.297264</td>\n",
       "      <td>0.396226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[strong, punish]</td>\n",
       "      <td>0.227494</td>\n",
       "      <td>0.328510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[sword, punish]</td>\n",
       "      <td>0.255758</td>\n",
       "      <td>0.281778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[sword, leviathan]</td>\n",
       "      <td>0.296781</td>\n",
       "      <td>0.449484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[punish, leviathan]</td>\n",
       "      <td>0.328893</td>\n",
       "      <td>0.385581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[punish, piercing]</td>\n",
       "      <td>0.123548</td>\n",
       "      <td>0.239331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[punish, serpent]</td>\n",
       "      <td>0.158319</td>\n",
       "      <td>0.133861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[piercing, serpent]</td>\n",
       "      <td>0.312624</td>\n",
       "      <td>0.424226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[piercing, leviathan]</td>\n",
       "      <td>0.236635</td>\n",
       "      <td>0.379963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[crooked, serpent]</td>\n",
       "      <td>0.336332</td>\n",
       "      <td>0.376844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[crooked, shall]</td>\n",
       "      <td>0.147340</td>\n",
       "      <td>0.316489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[crooked, slay]</td>\n",
       "      <td>0.214628</td>\n",
       "      <td>0.346844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word_pairs  item_dis  latent_dis\n",
       "0         [genesis, path]  0.146657    0.321490\n",
       "1           [path, shine]  0.172554    0.342823\n",
       "2          [shine, think]  0.254291    0.338092\n",
       "3          [think, hoary]  0.180608    0.329136\n",
       "4       [hoary, prepared]  0.033687    0.208438\n",
       "5             [day, sore]  0.340400    0.352394\n",
       "6          [sore, strong]  0.298166    0.409027\n",
       "7           [sore, sword]  0.190398    0.215757\n",
       "8         [strong, sword]  0.297264    0.396226\n",
       "9        [strong, punish]  0.227494    0.328510\n",
       "10        [sword, punish]  0.255758    0.281778\n",
       "11     [sword, leviathan]  0.296781    0.449484\n",
       "12    [punish, leviathan]  0.328893    0.385581\n",
       "13     [punish, piercing]  0.123548    0.239331\n",
       "14      [punish, serpent]  0.158319    0.133861\n",
       "15    [piercing, serpent]  0.312624    0.424226\n",
       "16  [piercing, leviathan]  0.236635    0.379963\n",
       "17     [crooked, serpent]  0.336332    0.376844\n",
       "18       [crooked, shall]  0.147340    0.316489\n",
       "19        [crooked, slay]  0.214628    0.346844"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
