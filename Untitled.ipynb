{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, topn=10):\n",
    "    #https://stackoverflow.com/questions/57697374/list-most-similar-words-in-spacy-in-pretrained-model\n",
    "    ms = nlp.vocab.vectors.most_similar(nlp(word).vector.reshape(1,nlp(word).vector.shape[0]), n=topn)\n",
    "    words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "    distances = ms[2]\n",
    "    return words, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = nlp.vocab.strings\n",
    "\n",
    "def latent_meaning_spacy(i, top_ = 10):\n",
    "    '''\n",
    "    INPUT: word tuple, topn\n",
    "    \n",
    "    OUTPUT: word tuple, the distance between the two original words, and the distance between the topn related words\n",
    "    '''\n",
    "    if(i[0] in vocab) & (i[1] in vocab):\n",
    "        \n",
    "        first_close, first_close_distances = most_similar(i[0], topn= top_)\n",
    "        print(first_close)\n",
    "        second_close, second_close_distances = most_similar(i[1], topn= top_)\n",
    "        print(second_close)\n",
    "        first_vec = nlp.vocab[i[0]].vector\n",
    "        second_vec = nlp.vocab[i[1]].vector\n",
    "        item_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + nlp.vocab[z].vector\n",
    "        \n",
    "        for z in second_close:\n",
    "            second_vec = second_vec +  nlp.vocab[z].vector\n",
    "        \n",
    "        first_vec = first_vec - nlp.vocab[i[0]].vector\n",
    "        second_vec = second_vec - nlp.vocab[i[1]].vector\n",
    "        \n",
    "        latent_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, latent_dis])\n",
    "\n",
    "\n",
    "def latent_meaning(i, model3):\n",
    "    \n",
    "    if(i[0] in model3.wv.vocab) & (i[1] in model3.wv.vocab):\n",
    "        first_close = list(model3.wv.most_similar(i[0], topn= 5))\n",
    "        print([i, first_close])\n",
    "        second_close = list(model3.wv.most_similar(i[1], topn= 5))\n",
    "        \n",
    "        first_vec = model3.wv.get_vector(i[0])\n",
    "        second_vec = model3.wv.get_vector(i[1])\n",
    "        \n",
    "        item_dis = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + model3.wv.get_vector(z[0])\n",
    "        for n in second_close:\n",
    "            #print(n)\n",
    "            second_vec = second_vec + model3.wv.get_vector(n[0])\n",
    "        \n",
    "        first_vec = first_vec - model3[i[0]]\n",
    "        second_vec = second_vec - model3[i[1]]\n",
    "        cos_sim = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, cos_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'the', 'thE', 'oF', 'of', 'Of', 'Entire', 'entire', 'OnE', 'one']\n",
      "['FELINE', 'cat', 'cats', 'FELINES', 'TABBY', 'KENNEL', 'dog', 'kitty', 'sanrio', 'pet']\n",
      "['iS', 'is', 'Is', 'Has', 'hAs', 'has', 'which', 'WHich', 'Be', 'be']\n",
      "['Very', 'VERy', 'very', 'QUITE', 'quite', 'EXTREMELY', 'Extremely', 'extremely', 'Really', 'really']\n",
      "['ugly', 'FUGLY', 'GHASTLY', 'awful', 'AWFUL', 'horrible', 'HORRIBLE', 'DISTASTEFUL', 'STUPID', 'stupid']\n"
     ]
    }
   ],
   "source": [
    "vocab = nlp.vocab.strings\n",
    "for n in test.split():\n",
    "    if n in vocab:\n",
    "        words, distances = most_similar(n)\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'the cat is very ugly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['take', 'tAke', 'Take', 'taking', 'Taking', 'TAKING', 'give', 'Give', 'GIve', 'get']\n",
      "['EASY', 'easy', 'quick', 'QUICKY', 'easier', 'EASIER', 'simple', 'SImple', 'easliy', 'easily']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('take', 'easy'), 0.53736985, 0.6324654]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_meaning_spacy(('take', 'easy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "#nltk mutual informativity\n",
    "\n",
    "#note from fritz with // around my edit\n",
    "#PMI is defined as //pmi(r,c)=log(P(r,c)/(P(r)*P(c)))//, with P(r,c) being the\n",
    "#probability of co-occurrence and P(r) and P(c) the probability of\n",
    "#occurrence of two words (estimated via frequency)\n",
    "\n",
    "#- I considered words as co-occurring if they occurred within a window\n",
    "#of 5 words:\n",
    "#no no yes yes yes yes target yes yes yes yes no no\n",
    "\n",
    "\n",
    "def make_nltktxt(komyagin):\n",
    "    #komyagin = komyagin.lower()\n",
    "    #holodkov = komyagin.split()\n",
    "    return(nltk.Text(komyagin))\n",
    "\n",
    "def make_ci(nltkText):\n",
    "    return(ConcordanceIndex(nltkText.tokens))\n",
    "\n",
    "#for some reason having difficulty subsetting by tokens instead of characters:: instead just use enough \n",
    "#characters to later be able to reliably get a 5 word radius\n",
    "\n",
    "def concordance_fancy(ci, word, width=150, lines=100):\n",
    "    \n",
    "    half_width =  (width - len(word) - 2) // 2\n",
    "    context = width // 4 # approx number of words of context\n",
    "    num = 5\n",
    "    problem_index = None\n",
    "    results = []\n",
    "    offsets = ci.offsets(word)\n",
    "    #vestigial code from trying to get tokens\n",
    "   # if len(offsets) > 1:\n",
    "    #    distances = [j-i for i, j in zip(offsets[:-1], offsets[1:])] \n",
    "     #   distances1 = list(enumerate(distances))\n",
    "      #  for i in distances1:\n",
    "       #     if i[1] < 10:\n",
    "        #        num = round((i[1]-1)/2)\n",
    "         #       problem_index = offsets[0]\n",
    "          #      print(problem_index)\n",
    "           #     print(ci._tokens[23])\n",
    "    if offsets:\n",
    "        #print([offsets, word])\n",
    "        for i in offsets:\n",
    "            query_word = ci._tokens[i]\n",
    "            #print(\"q_w\" + query_word)\n",
    "                # Find the context of query word.\n",
    "            left_context = ci._tokens[max(0, i - context) : i]\n",
    "            right_context = ci._tokens[i + 1 : i + context]\n",
    "           # print(right_context)\n",
    "                # Create the pretty lines with the query_word in the middle.\n",
    "            left_print = \" \".join(left_context)[-half_width:]\n",
    "            right_print = \" \".join(right_context)[:half_width]\n",
    "                # The WYSIWYG line of the concordance.\n",
    "            line_print = \" \".join([left_print, query_word, right_print])\n",
    "                # Create the ConcordanceLine\n",
    "            results.append(line_print)\n",
    "    return ([num, results, problem_index])\n",
    "\n",
    "#gathers all contexts of the word in results\n",
    "def mutual_informativity(ci, target_word, target_word2, total_count):\n",
    "    results0 = []\n",
    " #gets context around a target word\n",
    "    concordance1 = concordance_fancy(ci, target_word)\n",
    "    num = 5\n",
    "    #print(num)\n",
    " #takes this context and reshapes it into a -5 to + 5 window\n",
    "    for i in concordance1[1]:\n",
    "        #print(i)\n",
    "        n = i.split()\n",
    "        for z in n:\n",
    "            results0.append(z)\n",
    "    token_fix = list(enumerate(results0))\n",
    "    results1 = []\n",
    "#modifications necessary to account for words at the start and end of corpus\n",
    "    for z in token_fix:\n",
    "        if z[1] == target_word:\n",
    "            if z[0] < (num):\n",
    "                cat = results0[0:(z[0]+num)]\n",
    "                results1.append(cat)\n",
    "            elif z[0] > (len(results0) - 1):\n",
    "                cat = results0[(z[0] -num):(len(results0)-1)]\n",
    "                results1.append(cat)\n",
    "            else: \n",
    "                cat = results0[(z[0] -num):(z[0] + num)]\n",
    "                results1.append(cat)\n",
    "#reshaping list into format that can be more quickly processed    \n",
    "    results = []\n",
    "    for i in results1:\n",
    "        for n in i:\n",
    "            results.append(n)\n",
    "#this will return the count of target_word2 in the vicinity of target_word1\n",
    "    prob_numx = Counter(results)\n",
    "    prob_num = prob_numx[target_word2]\n",
    "    P_rc = prob_num/total_count\n",
    "    P_r = (count(ci, target_word))/total_count\n",
    "    P_c = (count(ci, target_word2))/total_count\n",
    " #checking for indexing errors and overlap errors.\n",
    "#fix to overlap is a bit of a hack fix, most notable problems are double counting \n",
    "#so if number of cooccurances is greater than number of times a word appears in \n",
    "#a document, then it replaces cooccurance with the total number of times\n",
    "#except in the case that this is an odd number (since it's not being double-counted then,\n",
    "#it's being double counted once and has another appearance)\n",
    "#odd numbers, is P_c - 1/total. \n",
    "#this still may cause some errors\n",
    "    if P_rc == 0:\n",
    "        print([num, concordance1, \"ERROR\"])\n",
    "        return ([target_word, target_word2, \"ERROR\"])\n",
    "    elif P_rc > P_c:\n",
    "        rounded = int(P_rc/P_c)\n",
    "        nr = P_rc/P_c\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c - 1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    elif P_rc > P_r:\n",
    "        rounded = int(P_rc/P_r)\n",
    "        nr = P_rc/P_r\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c -1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    else:\n",
    "        mutinf = math.log10(P_rc/(P_r*P_c))\n",
    "    return ([target_word, target_word2, mutinf])\n",
    "\n",
    "def count(ci, word):\n",
    "   \n",
    "    offsets = ci.offsets(word)\n",
    "\n",
    "    return len(offsets)\n",
    "from nltk import ConcordanceIndex\n",
    "import math\n",
    "def pmi(text):\n",
    "    text0 = text.lower()\n",
    "    text1 = text.split()\n",
    "    text1 = [i for i in text1 if i not in stop_words]\n",
    "    total_count = len(text1)\n",
    "    step_1 = make_nltktxt(text1)\n",
    "    step_2 = make_ci(step_1)\n",
    "    pmi_list = []\n",
    "    text2 = []\n",
    "    text2 = [i for i in text1 if i not in text2]\n",
    "    text3 = list(enumerate(text2))\n",
    "    index = 0\n",
    "    #realistically only words that at some point occur in a 3 word window are really worth looking at esp with these short texts\n",
    "    for i in text3:\n",
    "        if i[0] < (len(text3)-1):\n",
    "            #print(text2[i[0] + 1])\n",
    "            item = mutual_informativity(step_2, i[1], text3[i[0] + 1][1], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        if i[0] < (len(text3)-2):\n",
    "            #print(text2[i[0] + 1])\n",
    "            item = mutual_informativity(step_2, i[1], text3[i[0] + 2][1], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)        \n",
    "        if i[0] < (len(text3)-3):\n",
    "            #print(text2[i[0] + 1])\n",
    "            item = mutual_informativity(step_2, i[1], text3[i[0] + 3][1], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        else:\n",
    "            return(pmi_list)\n",
    "#just to allow for sorting by actual pmi index\n",
    "def takeSecond(elem):\n",
    "    #print(elem[2])\n",
    "    return elem[2]\n",
    "def strOnly(elem):\n",
    "   # print(elem[0])\n",
    "    \n",
    "    return elem[0]\n",
    "\n",
    "def strOnly2(elem):\n",
    "   # print(elem[0][0])\n",
    "    \n",
    "    return elem[0][0]\n",
    "\n",
    "def pmi_clean(text):\n",
    "    n = pmi(text)\n",
    "    sort1 = []\n",
    "    for i in n:\n",
    "        y = i[:2]\n",
    "        y.sort(key = strOnly)\n",
    "        x = [y[0], y[1], i[2]]\n",
    "        sort1.append(x)\n",
    "    q = sort1\n",
    "    #print(q)\n",
    "    q.sort(key = strOnly2)\n",
    "   # print(q)\n",
    "    k = list(enumerate(q))\n",
    "    pmi_cl = []\n",
    "    dumb_check = []\n",
    "    for z in k:\n",
    "        n = [z[1][0], z[1][1]]\n",
    "        if ((z[0]) < (len(k) -1)) and (n not in dumb_check):\n",
    "            #if((z[1][0] == k[z[0] + 1][1][0]) and (z[1][1] == k[z[0] + 1][1][1])) or ((z[1][0] == k[z[0] + 1][1][1]) and (z[1][1] == k[z[0] + 1][1][0])):\n",
    "            dumb_check.append(n)\n",
    "            #pmi_cl.append(k[z[0] + 1][1])\n",
    "            #elif (z[1] not in pmi_cl) and (n not in dumb_check):\n",
    "             #   dumb_check.append(n)\n",
    "            #  pmi_cl.append(z[1])\n",
    "        #elif (z[1] not in pmi_cl) and (n not in dumb_check):\n",
    "         #   dumb_check.append(n)\n",
    "            pmi_cl.append(z[1])\n",
    "    return pmi_cl\n",
    "            \n",
    "\n",
    "def pmi_high(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = True)\n",
    "    return cat[:n]\n",
    "\n",
    "def pmi_low(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = False)\n",
    "    return cat[:n]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
