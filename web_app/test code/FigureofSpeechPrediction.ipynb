{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import ConcordanceIndex\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable = ['tagger', 'parser'])\n",
    "vocab = nlp.vocab.strings\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "from re import sub\n",
    "punctuation = punctuation +'”'+'“'+'’' + '—' + '’' + '‘' +'0123456789'\n",
    "\n",
    "import progressbar\n",
    "\n",
    "test_doc = open(\"./test_doc.txt\", \"r\", encoding = \"utf-8\")\n",
    "test_doc = test_doc.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vectors = 15000  # number of vectors to keep\n",
    "removed_words = nlp.vocab.prune_vectors(n_vectors)\n",
    "\n",
    "assert len(nlp.vocab.vectors) <= n_vectors  # unique vectors have been pruned\n",
    "assert nlp.vocab.vectors.n_keys > n_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [i for i in nlp.vocab.vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( nlp.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'en'\n",
    "pipeline = ['tagger', 'parser', 'ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_data = nlp.to_bytes('reduced_model')\n",
    "lang = nlp.meta[\"lang\"]  # \"en\"\n",
    "pipeline = nlp.meta[\"pipeline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0f62f9844ff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reduced_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mfrom_bytes\u001b[1;34m(self, bytes_data, exclude, disable, **kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m             )\n\u001b[0;32m    997\u001b[0m         \u001b[0mexclude\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_serialization_exclude\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeserializers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mfrom_bytes\u001b[1;34m(bytes_data, setters, exclude)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfrom_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrsly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsgpack_loads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msetters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;31m# Split to support file names like meta.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\srsly\\_msgpack_api.py\u001b[0m in \u001b[0;36mmsgpack_loads\u001b[1;34m(data, use_list)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# msgpack-python docs suggest disabling gc before unpacking large messages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmsgpack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\srsly\\msgpack\\__init__.py\u001b[0m in \u001b[0;36munpackb\u001b[1;34m(packed, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mobject_hook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object_hook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'object_hook'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decode_numpy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_unpackb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_unpacker.pyx\u001b[0m in \u001b[0;36msrsly.msgpack._unpacker.unpackb\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_unpacker.pyx\u001b[0m in \u001b[0;36msrsly.msgpack._unpacker.get_data_from_buffer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(lang)\n",
    "for pipe_name in pipeline:\n",
    "    pipe = nlp.create_pipe(pipe_name)\n",
    "    nlp.add_pipe(pipe)\n",
    "nlp.from_bytes('reduced_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['whom','hast','thou','therein', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, topn=10):\n",
    "    #https://stackoverflow.com/questions/57697374/list-most-similar-words-in-spacy-in-pretrained-model\n",
    "    ms = nlp.vocab.vectors.most_similar(nlp(word).vector.reshape(1,nlp(word).vector.shape[0]), n=topn)\n",
    "    words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "    distances = ms[2]\n",
    "    return words, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def latent_meaning_spacy(i, top_ = 10):\n",
    "    '''\n",
    "    INPUT: word tuple, topn\n",
    "    \n",
    "    OUTPUT: word tuple, the distance between the two original words, and the distance between the topn related words\n",
    "    '''\n",
    "    if(i[0] in vocab) & (i[1] in vocab):\n",
    "        \n",
    "        first_close, first_close_distances = most_similar(i[0], topn= top_)\n",
    "        second_close, second_close_distances = most_similar(i[1], topn= top_)\n",
    "        first_vec = nlp.vocab[i[0]].vector\n",
    "        second_vec = nlp.vocab[i[1]].vector\n",
    "        item_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + nlp.vocab[z].vector\n",
    "        \n",
    "        for z in second_close:\n",
    "            second_vec = second_vec +  nlp.vocab[z].vector\n",
    "        \n",
    "        first_vec = first_vec - nlp.vocab[i[0]].vector\n",
    "        second_vec = second_vec - nlp.vocab[i[1]].vector\n",
    "        \n",
    "        latent_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, latent_dis])\n",
    "    else:\n",
    "        return([None, None, None])\n",
    "\n",
    "\n",
    "def latent_meaning(i, model3):\n",
    "    \n",
    "    if(i[0] in model3.wv.vocab) & (i[1] in model3.wv.vocab):\n",
    "        first_close = list(model3.wv.most_similar(i[0], topn= 5))\n",
    "        second_close = list(model3.wv.most_similar(i[1], topn= 5))\n",
    "        \n",
    "        first_vec = model3.wv.get_vector(i[0])\n",
    "        second_vec = model3.wv.get_vector(i[1])\n",
    "        \n",
    "        item_dis = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + model3.wv.get_vector(z[0])\n",
    "        for n in second_close:\n",
    "            second_vec = second_vec + model3.wv.get_vector(n[0])\n",
    "        \n",
    "        first_vec = first_vec - model3[i[0]]\n",
    "        second_vec = second_vec - model3[i[1]]\n",
    "        cos_sim = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, cos_sim])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PMI is defined as //pmi(r,c)=log(P(r,c)/(P(r)*P(c)))//, with P(r,c) being the\n",
    "#probability of co-occurrence and P(r) and P(c) the probability of\n",
    "#occurrence of two words (estimated via frequency)\n",
    "\n",
    "#- I considered words as co-occurring if they occurred within a window\n",
    "#of 5 words:\n",
    "#no no yes yes yes yes target yes yes yes yes no no\n",
    "\n",
    "\n",
    "def most_similar(word, topn=20):\n",
    "    #https://stackoverflow.com/questions/57697374/list-most-similar-words-in-spacy-in-pretrained-model\n",
    "    ms = nlp.vocab.vectors.most_similar(nlp(word).vector.reshape(1,nlp(word).vector.shape[0]), n=topn)\n",
    "    ms = list(zip(ms[0][0], ms[1][0], ms[2][0]))\n",
    "    print(ms)\n",
    "    ms = [i for i in ms if str(nlp.vocab.strings[i[0]]).lower() != word]\n",
    "    ms = [i for i in ms if str(nlp.vocab.strings[i[0]]).lower() + 's' != word]\n",
    "    ms = [i for i in ms if str(nlp.vocab.strings[i[0]]).lower() + 'es' != word]\n",
    "    words = [nlp.vocab.strings[w[0]] for w in ms]\n",
    "    distances = [i[2] for i in ms]\n",
    "    return words, distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lock': ('lock', 1.0),\n",
       " 'sourced': ('locally', 1.0),\n",
       " 'swell': ('winds', 1.0),\n",
       " 'Geez': ('Jeez', 1.0),\n",
       " 'flops': ('flop', 1.0),\n",
       " 'RE': ('Re', 1.0),\n",
       " 'speeches': ('speech', 1.0),\n",
       " 'likewise': ('Likewise', 1.0),\n",
       " 'reservations': ('Hotel', 1.0),\n",
       " 'Tigers': ('lions', 1.0),\n",
       " 'Durant': ('Bryant', 1.0),\n",
       " 'Possible': ('possible', 1.0),\n",
       " 'terrific': ('Fantastic', 1.0),\n",
       " 'spins': ('spin', 1.0),\n",
       " 'simulate': ('mimic', 1.0),\n",
       " 'sampling': ('samples', 1.0),\n",
       " 'Tour': ('tour', 1.0),\n",
       " 'Pikachu': ('Pokemon', 1.0),\n",
       " 'midwest': ('Midwest', 1.0),\n",
       " 'wisely': ('accordingly', 1.0),\n",
       " '8-10': ('6-8', 1.0),\n",
       " 'watermelon': ('pineapple', 1.0),\n",
       " 'lackluster': ('underwhelming', 1.0),\n",
       " 'glimpse': ('peek', 1.0),\n",
       " 'saddest': ('happiest', 1.0),\n",
       " 'subscriber': ('subscribers', 1.0),\n",
       " 'RIOT': ('riot', 1.0),\n",
       " 'islam': ('Islam', 1.0),\n",
       " 'PS1': ('PS2', 1.0),\n",
       " 'survivability': ('dps', 1.0),\n",
       " 'devastated': ('devastating', 1.0),\n",
       " 'Trading': ('trading', 1.0),\n",
       " 'operators': ('operator', 1.0),\n",
       " 'Voldemort': ('Harry', 1.0),\n",
       " 'stains': ('stain', 1.0),\n",
       " 'sacks': ('sack', 1.0),\n",
       " 'Smite': ('smite', 1.0),\n",
       " 'informing': ('inform', 1.0),\n",
       " 'Dominion': ('kingdom', 1.0),\n",
       " 'arrangements': ('arrangement', 1.0),\n",
       " 'typos': ('typo', 1.0),\n",
       " 'roses': ('flowers', 1.0),\n",
       " 'referee': ('refs', 1.0),\n",
       " 'longevity': ('lifespan', 1.0),\n",
       " 'mechanically': ('sufficiently', 1.0),\n",
       " 'Finish': ('finish', 1.0),\n",
       " 'jaded': ('cynical', 1.0),\n",
       " 'Wut': ('wut', 1.0),\n",
       " 'porno': ('Porn', 1.0),\n",
       " 'Communism': ('communism', 1.0),\n",
       " 'Originally': ('originally', 1.0),\n",
       " 'myriad': ('multitude', 1.0),\n",
       " 'blacked': ('censored', 1.0),\n",
       " 'transitioning': ('transition', 1.0),\n",
       " 'smelly': ('smelling', 1.0),\n",
       " 'columns': ('column', 1.0),\n",
       " 'machinery': ('equipment', 1.0),\n",
       " 'Radiohead': ('Beatles', 1.0),\n",
       " 'hysterical': ('Hilarious', 1.0),\n",
       " 'reactionary': ('bigoted', 1.0),\n",
       " 'elimination': ('eliminated', 1.0),\n",
       " 'compensated': ('compensate', 1.0),\n",
       " 'executives': ('managers', 1.0),\n",
       " 'Flag': ('flag', 1.0),\n",
       " 'Bingo': ('casino', 1.0),\n",
       " 'PCI': ('usb', 1.0),\n",
       " 'choir': ('chorus', 1.0),\n",
       " 'retention': ('effectiveness', 1.0),\n",
       " 'pokes': ('poking', 1.0),\n",
       " 'Excel': ('excel', 1.0),\n",
       " 'poisoned': ('poison', 1.0),\n",
       " 'topped': ('tops', 1.0),\n",
       " 'cutest': ('adorable', 1.0),\n",
       " 'mounting': ('Mount', 1.0),\n",
       " 'Tag': ('tag', 1.0),\n",
       " 'mixer': ('mixing', 1.0),\n",
       " 'Heres': ('heres', 1.0),\n",
       " 'Diet': ('diet', 1.0),\n",
       " 'Shaq': ('LeBron', 1.0),\n",
       " 'imminent': ('inevitable', 1.0),\n",
       " 'Sleeping': ('sleeping', 1.0),\n",
       " 'mids': ('highs', 1.0),\n",
       " 'menos': ('porque', 1.0),\n",
       " 'awkwardness': ('embarrassment', 1.0),\n",
       " 'anthem': ('Song', 1.0),\n",
       " 'hitter': ('pitcher', 1.0),\n",
       " 'Venus': ('Moon', 1.0),\n",
       " 'cursor': ('pointer', 1.0),\n",
       " 'skulle': ('så', 1.0),\n",
       " 'kindle': ('Kindle', 1.0),\n",
       " 'blackout': ('midnight', 1.0),\n",
       " 'waterproof': ('durable', 1.0),\n",
       " 'handheld': ('portable', 1.0),\n",
       " 'crackers': ('peanut', 1.0),\n",
       " 'factually': ('untrue', 1.0),\n",
       " 'Baseball': ('baseball', 1.0),\n",
       " 'pitches': ('pitch', 1.0),\n",
       " 'behold': ('unto', 1.0),\n",
       " 'Taken': ('taken', 1.0),\n",
       " 'remedy': ('cure', 1.0),\n",
       " 'cleansing': ('wash', 1.0),\n",
       " 'redirect': ('disable', 1.0),\n",
       " 'Called': ('called', 1.0),\n",
       " 'nick': ('Nick', 1.0),\n",
       " 'christianity': ('Christianity', 1.0),\n",
       " 'synonymous': ('regarded', 1.0),\n",
       " 'Nobel': ('Oscar', 1.0),\n",
       " 'bogus': ('misleading', 1.0),\n",
       " 'Ender': ('gank', 1.0),\n",
       " 'mantra': ('chant', 1.0),\n",
       " 'gyms': ('gym', 1.0),\n",
       " 'montage': ('footage', 1.0),\n",
       " 'capturing': ('capture', 1.0),\n",
       " 'Million': ('million', 1.0),\n",
       " 'weirdo': ('psycho', 1.0),\n",
       " 'Exchange': ('exchange', 1.0),\n",
       " 'Bass': ('bass', 1.0),\n",
       " 'Teams': ('teams', 1.0),\n",
       " 'puede': ('porque', 1.0),\n",
       " 'Teach': ('teach', 1.0),\n",
       " 'discrete': ('linear', 1.0),\n",
       " 'bonding': ('Bond', 1.0),\n",
       " '6000': ('4000', 1.0),\n",
       " 'pinky': ('finger', 1.0),\n",
       " 'nvidia': ('Nvidia', 1.0),\n",
       " 'tattooed': ('tattoos', 1.0),\n",
       " 'adverse': ('risks', 1.0),\n",
       " 'aroused': ('lust', 1.0),\n",
       " 'NT': ('nt', 1.0),\n",
       " 'lego': ('Lego', 1.0),\n",
       " 'plethora': ('multitude', 1.0),\n",
       " 'Chip': ('chip', 1.0),\n",
       " 'apathy': ('laziness', 1.0),\n",
       " 'Wolves': ('wolves', 1.0),\n",
       " 'Joke': ('joke', 1.0),\n",
       " 'baffled': ('horrified', 1.0),\n",
       " 'Active': ('active', 1.0),\n",
       " 'Blink': ('blink', 1.0),\n",
       " 'curtain': ('window', 1.0),\n",
       " 'Soccer': ('soccer', 1.0),\n",
       " 'graffiti': ('Art', 1.0),\n",
       " 'GTFO': ('faggot', 1.0),\n",
       " 'Por': ('por', 1.0),\n",
       " 'candles': ('candle', 1.0),\n",
       " 'emperor': ('Emperor', 1.0),\n",
       " 'depicted': ('portrayed', 1.0),\n",
       " 'ign': ('IGN', 1.0),\n",
       " 'Libertarians': ('libertarians', 1.0),\n",
       " 'latin': ('Latin', 1.0),\n",
       " 'qualifying': ('qualify', 1.0),\n",
       " 'radically': ('fundamentally', 1.0),\n",
       " 'Senator': ('senate', 1.0),\n",
       " 'rigorous': ('thorough', 1.0),\n",
       " 'elephants': ('elephant', 1.0),\n",
       " 'smear': ('discredit', 1.0),\n",
       " 'indifferent': ('oblivious', 1.0),\n",
       " 'enables': ('allows', 1.0),\n",
       " 'refusal': ('refusing', 1.0),\n",
       " 'usernames': ('username', 1.0),\n",
       " 'yields': ('yield', 1.0),\n",
       " 'revolves': ('focuses', 1.0),\n",
       " 'premiums': ('Insurance', 1.0),\n",
       " 'Islands': ('islands', 1.0),\n",
       " 'bounced': ('bouncing', 1.0),\n",
       " 'hoo': ('Boo', 1.0),\n",
       " 'Skill': ('skill', 1.0),\n",
       " 'bob': ('Bob', 1.0),\n",
       " 'weirdly': ('Oddly', 1.0),\n",
       " 'WORK': ('Work', 1.0),\n",
       " 'Politics': ('politics', 1.0),\n",
       " 'approx': ('approximately', 1.0),\n",
       " 'hovering': ('hover', 1.0),\n",
       " 'dolls': ('doll', 1.0),\n",
       " 'Nikon': ('Canon', 1.0),\n",
       " 'psychologically': ('emotionally', 1.0),\n",
       " '81': ('82', 1.0),\n",
       " 'nutritional': ('nutrition', 1.0),\n",
       " 'formerly': ('Former', 1.0),\n",
       " 'Metallica': ('Beatles', 1.0),\n",
       " 'futile': ('pointless', 1.0),\n",
       " 'IC': ('TR', 1.0),\n",
       " 'neighbour': ('neighbor', 1.0),\n",
       " 'analogous': ('derive', 1.0),\n",
       " 'Gronk': ('neckbeard', 1.0),\n",
       " 'Idea': ('idea', 1.0),\n",
       " 'seeks': ('seeking', 1.0),\n",
       " 'welcomed': ('welcoming', 1.0),\n",
       " 'sugars': ('Sugar', 1.0),\n",
       " 'starring': ('actor', 1.0),\n",
       " 'obtaining': ('obtain', 1.0),\n",
       " 'fumble': ('touchdown', 1.0),\n",
       " 'blinded': ('Blind', 1.0),\n",
       " '9gag': ('4chan', 1.0),\n",
       " 's/he': ('he/she', 1.0),\n",
       " 'Dig': ('dig', 1.0),\n",
       " 'bundles': ('bundle', 1.0),\n",
       " 'Ralph': ('Calvin', 1.0),\n",
       " 'stakes': ('stake', 1.0),\n",
       " 'colonial': ('imperial', 1.0),\n",
       " 'Arcade': ('arcade', 1.0),\n",
       " 'maxing': ('maxed', 1.0),\n",
       " 'Stan': ('Keith', 1.0),\n",
       " 'Futurama': ('Simpsons', 1.0),\n",
       " 'Matthews': ('Dave', 1.0),\n",
       " 'doubling': ('increasing', 1.0),\n",
       " 'Wanted': ('wanted', 1.0),\n",
       " 'asylum': ('refugees', 1.0),\n",
       " 'assemble': ('gather', 1.0),\n",
       " 'Culture': ('culture', 1.0),\n",
       " 'pitchers': ('pitcher', 1.0),\n",
       " 'advancing': ('advancement', 1.0),\n",
       " 'banged': ('fucked', 1.0),\n",
       " 'everyones': ('elses', 1.0),\n",
       " 'relieve': ('discomfort', 1.0),\n",
       " 'limp': ('stiff', 1.0),\n",
       " 'F.': ('E.', 1.0),\n",
       " 'Lose': ('lose', 1.0),\n",
       " 'Reigns': ('reign', 1.0),\n",
       " 'mens': ('Ladies', 1.0),\n",
       " 'Sauce': ('sauce', 1.0),\n",
       " 'Buddy': ('buddy', 1.0),\n",
       " 'signatures': ('signature', 1.0),\n",
       " 'Prison': ('prison', 1.0),\n",
       " 'salute': ('honor', 1.0),\n",
       " 'bane': ('Bane', 1.0),\n",
       " 'spraying': ('sprayed', 1.0),\n",
       " 'Ramsey': ('Gordon', 1.0),\n",
       " 'Jr': ('Jr.', 1.0),\n",
       " 'Crime': ('crime', 1.0),\n",
       " 'perpetual': ('eternal', 1.0),\n",
       " 'inflammatory': ('immune', 1.0),\n",
       " 'connotation': ('derogatory', 1.0),\n",
       " 'perceptions': ('perception', 1.0),\n",
       " 'contracted': ('contract', 1.0),\n",
       " 'endorse': ('oppose', 1.0),\n",
       " 'BUY': ('Buy', 1.0),\n",
       " 'skirts': ('skirt', 1.0),\n",
       " 'PMs': ('cramps', 1.0),\n",
       " 'journals': ('journal', 1.0),\n",
       " 'scheduling': ('schedules', 1.0),\n",
       " 'badges': ('badge', 1.0),\n",
       " 'injure': ('harm', 1.0),\n",
       " 'lakes': ('Lake', 1.0),\n",
       " 'airing': ('aired', 1.0),\n",
       " 'RBs': ('QBs', 1.0),\n",
       " 'YA': ('Ya', 1.0),\n",
       " 'replaces': ('replacing', 1.0),\n",
       " 'Gross': ('gross', 1.0),\n",
       " 'afaik': ('AFAIK', 1.0),\n",
       " 'san': ('San', 1.0),\n",
       " 'simplify': ('simpler', 1.0),\n",
       " 'froze': ('freeze', 1.0),\n",
       " 'LEDs': ('lights', 1.0),\n",
       " 'Files': ('files', 1.0),\n",
       " 'Cheney': ('Clinton', 1.0),\n",
       " 'burner': ('stove', 1.0),\n",
       " 'throats': ('throat', 1.0),\n",
       " 'abort': ('interrupt', 1.0),\n",
       " 'SUV': ('Toyota', 1.0),\n",
       " 'monks': ('monk', 1.0),\n",
       " 'booted': ('boot', 1.0),\n",
       " 'tangent': ('angle', 1.0),\n",
       " 'policing': ('enforcement', 1.0),\n",
       " 'dan': ('Dan', 1.0),\n",
       " 'deterrent': ('deter', 1.0),\n",
       " 'obliged': ('compelled', 1.0),\n",
       " 'Samuel': ('Joseph', 1.0),\n",
       " 'weaponry': ('weapons', 1.0),\n",
       " 'BDSM': ('fetish', 1.0),\n",
       " 'oak': ('pine', 1.0),\n",
       " 'Harley': ('motorcycle', 1.0),\n",
       " 'climax': ('orgasm', 1.0),\n",
       " 'booking': ('booked', 1.0),\n",
       " 'abiding': ('abide', 1.0),\n",
       " '7/10': ('8/10', 1.0),\n",
       " 'reinforced': ('reinforce', 1.0),\n",
       " 'unofficial': ('Official', 1.0),\n",
       " 'wield': ('wielding', 1.0),\n",
       " 'Carlos': ('Jose', 1.0),\n",
       " 'pounding': ('banging', 1.0),\n",
       " 'dismissive': ('condescending', 1.0),\n",
       " 'announcements': ('announcement', 1.0),\n",
       " 'critic': ('critics', 1.0),\n",
       " '1970': ('1980', 1.0),\n",
       " 'focal': ('lens', 1.0),\n",
       " 'texas': ('Texas', 1.0),\n",
       " 'PSP': ('ps3', 1.0),\n",
       " 'spontaneous': ('induced', 1.0),\n",
       " 'compilation': ('compiled', 1.0),\n",
       " 'reflective': ('reflection', 1.0),\n",
       " 'Obvious': ('obvious', 1.0),\n",
       " 'Stafford': ('Virginia', 1.0),\n",
       " 'submissive': ('kinky', 1.0),\n",
       " 'Slayer': ('Demon', 1.0),\n",
       " 'floppy': ('disks', 1.0),\n",
       " 'surgeries': ('surgery', 1.0),\n",
       " 'outsider': ('cynical', 1.0),\n",
       " 'nether': ('flesh', 1.0),\n",
       " '30,000': ('20,000', 1.0),\n",
       " 'alleviate': ('mitigate', 1.0),\n",
       " 'twists': ('Twist', 1.0),\n",
       " '16GB': ('8GB', 1.0),\n",
       " 'Deadpool': ('Avengers', 1.0),\n",
       " 'auf': ('mit', 1.0),\n",
       " 'Gandalf': ('Hobbit', 1.0),\n",
       " 'emerge': ('evolve', 1.0),\n",
       " 'Net': ('net', 1.0),\n",
       " 'sneeze': ('fart', 1.0),\n",
       " 'dvd': ('DVD', 1.0),\n",
       " 'discriminated': ('discriminate', 1.0),\n",
       " 'tenants': ('tenant', 1.0),\n",
       " '00': ('08', 1.0),\n",
       " 'pudding': ('Pie', 1.0),\n",
       " 'Pls': ('pls', 1.0),\n",
       " 'twilight': ('Twilight', 1.0),\n",
       " 'generalizing': ('generalize', 1.0),\n",
       " 'narcissistic': ('delusional', 1.0),\n",
       " 'DMT': ('MDMA', 1.0),\n",
       " 'Console': ('console', 1.0),\n",
       " 'adjacent': ('located', 1.0),\n",
       " 'Thief': ('thief', 1.0),\n",
       " 'john': ('John', 1.0),\n",
       " 'legalize': ('legalized', 1.0),\n",
       " 'freed': ('escaped', 1.0),\n",
       " 'massacre': ('atrocities', 1.0),\n",
       " 'oatmeal': ('cereal', 1.0),\n",
       " 'sovereignty': ('sovereign', 1.0),\n",
       " 'Cobb': ('Georgia', 1.0),\n",
       " 'flashback': ('flashbacks', 1.0),\n",
       " 'promos': ('promo', 1.0),\n",
       " 'NHS': ('uk', 1.0),\n",
       " 'Jupiter': ('Apollo', 1.0),\n",
       " 'Cowboy': ('cowboy', 1.0),\n",
       " 'agility': ('endurance', 1.0),\n",
       " 'Nevertheless': ('nonetheless', 1.0),\n",
       " 'discontinued': ('purchased', 1.0),\n",
       " 'HM': ('Hm', 1.0),\n",
       " 'narrator': ('protagonist', 1.0),\n",
       " 'laps': ('lap', 1.0),\n",
       " 'Malcolm': ('Keith', 1.0),\n",
       " '74': ('68', 1.0),\n",
       " 'Finn': ('Jake', 1.0),\n",
       " 'assertions': ('assertion', 1.0),\n",
       " 'caveat': ('Disclaimer', 1.0),\n",
       " 'tantrum': ('temper', 1.0),\n",
       " 'jokingly': ('politely', 1.0),\n",
       " 'Realistically': ('realistically', 1.0),\n",
       " 'grape': ('fruit', 1.0),\n",
       " 'Near': ('near', 1.0),\n",
       " 'stitch': ('stitches', 1.0),\n",
       " 'luggage': ('baggage', 1.0),\n",
       " 'paleo': ('vegan', 1.0),\n",
       " 'Catholicism': ('Christianity', 1.0),\n",
       " '.22': ('rifle', 1.0),\n",
       " 'BW': ('DB', 1.0),\n",
       " 'gmail': ('Yahoo', 1.0),\n",
       " 'relapse': ('chronic', 1.0),\n",
       " 'disappearing': ('disappear', 1.0),\n",
       " 'boobies': ('titties', 1.0),\n",
       " 'moisturizer': ('lotion', 1.0),\n",
       " 'Subaru': ('Toyota', 1.0),\n",
       " 'Lannister': ('Tyrion', 1.0),\n",
       " 'Ummm': ('umm', 1.0),\n",
       " 'punishments': ('punishment', 1.0),\n",
       " 'dishwasher': ('fridge', 1.0),\n",
       " 'Monty': ('Brian', 1.0),\n",
       " 'breeder': ('breed', 1.0),\n",
       " 'RUN': ('Run', 1.0),\n",
       " 'correctness': ('ideology', 1.0),\n",
       " 'i3': ('i5', 1.0),\n",
       " '73': ('67', 1.0),\n",
       " 'Dolphins': ('whales', 1.0),\n",
       " 'sequences': ('sequence', 1.0),\n",
       " 'bureaucracy': ('Government', 1.0),\n",
       " 'Shaw': ('Morris', 1.0),\n",
       " 'coated': ('coating', 1.0),\n",
       " 'Pros': ('pros', 1.0),\n",
       " 'Travis': ('Keith', 1.0),\n",
       " 'trope': ('cliche', 1.0),\n",
       " 'diminish': ('drastically', 1.0),\n",
       " 'cyber': ('hackers', 1.0),\n",
       " 'downsides': ('outweigh', 1.0),\n",
       " 'cholesterol': ('fats', 1.0),\n",
       " 'villagers': ('villages', 1.0),\n",
       " 'pedophilia': ('homosexuality', 1.0),\n",
       " 'Razor': ('razor', 1.0),\n",
       " 'hur': ('inte', 1.0),\n",
       " 'honour': ('honor', 1.0),\n",
       " 'Spam': ('SPAM', 1.0),\n",
       " 'interviewer': ('interview', 1.0),\n",
       " 'LA.': ('Hehe', 1.0),\n",
       " 'inserted': ('Insert', 1.0),\n",
       " 'relativity': ('Theory', 1.0),\n",
       " '5s': ('20s', 1.0),\n",
       " 'someplace': ('Somewhere', 1.0),\n",
       " 'rhymes': ('rhyme', 1.0),\n",
       " 'forests': ('Forest', 1.0),\n",
       " 'recharge': ('batteries', 1.0),\n",
       " 'virgins': ('Virgin', 1.0),\n",
       " 'barking': ('yelling', 1.0),\n",
       " 'Saturn': ('Toyota', 1.0),\n",
       " 'centralized': ('unified', 1.0),\n",
       " 'Knock': ('knock', 1.0),\n",
       " 'gettin': ('goin', 1.0),\n",
       " 'toothpaste': ('shampoo', 1.0),\n",
       " 'defendant': ('prosecution', 1.0),\n",
       " 'selfie': ('instagram', 1.0),\n",
       " 'Africans': ('Europeans', 1.0),\n",
       " 'sighs': ('Sigh', 1.0),\n",
       " 'cleans': ('removes', 1.0),\n",
       " 'Lily': ('flower', 1.0),\n",
       " 'deeds': ('deed', 1.0),\n",
       " 'reflex': ('stimulation', 1.0),\n",
       " 'Jenny': ('Lisa', 1.0),\n",
       " 'IBM': ('microsoft', 1.0),\n",
       " 'Bless': ('bless', 1.0),\n",
       " 'entails': ('involves', 1.0),\n",
       " 'illusions': ('illusion', 1.0),\n",
       " 'hai': ('Ho', 1.0),\n",
       " 'Barney': ('Fred', 1.0),\n",
       " 'Presumably': ('presumably', 1.0),\n",
       " 'archer': ('Archer', 1.0),\n",
       " 'attracts': ('attract', 1.0),\n",
       " '7.5': ('4.5', 1.0),\n",
       " 'apocalyptic': ('apocalypse', 1.0),\n",
       " 'Harvey': ('Richard', 1.0),\n",
       " 'screwdriver': ('wrench', 1.0),\n",
       " 'Westbrook': ('Bryant', 1.0),\n",
       " 'smartphones': ('smartphone', 1.0),\n",
       " 'cling': ('adhere', 1.0),\n",
       " 'capitalists': ('capitalist', 1.0),\n",
       " 'Ability': ('ability', 1.0),\n",
       " 'overused': ('cliche', 1.0),\n",
       " 'Torres': ('Suarez', 1.0),\n",
       " 'verge': ('Suddenly', 1.0),\n",
       " 'traced': ('originated', 1.0),\n",
       " 'mics': ('mic', 1.0),\n",
       " 'stalk': ('stems', 1.0),\n",
       " 'traders': ('trader', 1.0),\n",
       " 'cig': ('cigs', 1.0),\n",
       " 'rebates': ('incentives', 1.0),\n",
       " 'commuting': ('commute', 1.0),\n",
       " 'Wenger': ('Barca', 1.0),\n",
       " 'Philosophy': ('philosophy', 1.0),\n",
       " 'dept': ('Department', 1.0),\n",
       " 'fooling': ('pretending', 1.0),\n",
       " 'Mickey': ('Disney', 1.0),\n",
       " 'alcoholism': ('addiction', 1.0),\n",
       " 'funk': ('Jazz', 1.0),\n",
       " 'KD': ('DM', 1.0),\n",
       " 'hallucinations': ('nightmares', 1.0),\n",
       " 'dislikes': ('dislike', 1.0),\n",
       " 'Related': ('related', 1.0),\n",
       " 'separates': ('separating', 1.0),\n",
       " 'Oz': ('oz', 1.0),\n",
       " 'interval': ('intervals', 1.0),\n",
       " 'summons': ('summon', 1.0),\n",
       " 'intoxicated': ('Drunk', 1.0),\n",
       " 'Staff': ('staff', 1.0),\n",
       " 'jerky': ('sausage', 1.0),\n",
       " 'fonts': ('font', 1.0),\n",
       " 'intrinsic': ('inherent', 1.0),\n",
       " 'miserably': ('horribly', 1.0),\n",
       " 'Bluetooth': ('bluetooth', 1.0),\n",
       " 'watered': ('planted', 1.0),\n",
       " 'Screen': ('screen', 1.0),\n",
       " 'Bengals': ('Steelers', 1.0),\n",
       " 'NDP': ('UKIP', 1.0),\n",
       " 'creationism': ('Atheism', 1.0),\n",
       " 'Fate': ('fate', 1.0),\n",
       " 'catalyst': ('reaction', 1.0),\n",
       " 'mortar': ('bricks', 1.0),\n",
       " 'russia': ('Russia', 1.0),\n",
       " 'persecuted': ('oppressed', 1.0),\n",
       " 'announcers': ('commentators', 1.0),\n",
       " 'NOW.': ('LOL.', 1.0),\n",
       " 'vad': ('är', 1.0),\n",
       " 'airlines': ('airline', 1.0),\n",
       " 'replacements': ('replacement', 1.0),\n",
       " 'Rust': ('rust', 1.0),\n",
       " 'hex': ('wrench', 1.0),\n",
       " 'Account': ('account', 1.0),\n",
       " 'Remix': ('remix', 1.0),\n",
       " '76': ('78', 1.0),\n",
       " 'mmo': ('MMO', 1.0),\n",
       " 'Americas': ('america', 1.0),\n",
       " 'hull': ('boat', 1.0),\n",
       " 'Uncharted': ('Borderlands', 1.0),\n",
       " 'LEAST': ('least', 1.0),\n",
       " 'uterus': ('womb', 1.0),\n",
       " 'systematic': ('empirical', 1.0),\n",
       " 'grasping': ('grasp', 1.0),\n",
       " 'dass': ('auch', 1.0),\n",
       " 'slutty': ('slut', 1.0),\n",
       " 'ambition': ('desire', 1.0),\n",
       " 'proudly': ('proud', 1.0),\n",
       " 'subtly': ('subtle', 1.0),\n",
       " 'psychedelics': ('psychedelic', 1.0),\n",
       " 'corpses': ('corpse', 1.0),\n",
       " 'pdf': ('PDF', 1.0),\n",
       " 'Rank': ('rank', 1.0),\n",
       " 'CPUs': ('cpu', 1.0),\n",
       " 'mongering': ('misinformation', 1.0),\n",
       " 'Moral': ('moral', 1.0),\n",
       " 'Blame': ('blame', 1.0),\n",
       " 'Parliament': ('parliament', 1.0),\n",
       " 'walker': ('Walker', 1.0),\n",
       " 'wrestlers': ('wrestler', 1.0),\n",
       " 'socialize': ('interact', 1.0),\n",
       " 'Shows': ('shows', 1.0),\n",
       " 'blu': ('Blu', 1.0),\n",
       " 'trump': ('Donald', 1.0),\n",
       " 'Di': ('di', 1.0),\n",
       " 'retrieve': ('fetch', 1.0),\n",
       " 'tread': ('tire', 1.0),\n",
       " 'Treat': ('treat', 1.0),\n",
       " '2000s': ('90s', 1.0),\n",
       " 'M4': ('Glock', 1.0),\n",
       " 'Cover': ('cover', 1.0),\n",
       " 'EX': ('Ex', 1.0),\n",
       " 'acquiring': ('acquire', 1.0),\n",
       " 'AZ': ('Arizona', 1.0),\n",
       " 'ounces': ('ounce', 1.0),\n",
       " 'amazes': ('annoys', 1.0),\n",
       " 'idol': ('singer', 1.0),\n",
       " 'mucho': ('muy', 1.0),\n",
       " 'ID.': ('II.', 1.0),\n",
       " '760': ('780', 1.0),\n",
       " 'sims': ('Sims', 1.0),\n",
       " 'ASRock': ('Asus', 1.0),\n",
       " 'leaps': ('leap', 1.0),\n",
       " 'benchmark': ('performance', 1.0),\n",
       " 'FX': ('GeForce', 1.0),\n",
       " 'lemme': ('Uhh', 1.0),\n",
       " 'rephrase': ('clarify', 1.0),\n",
       " 'commits': ('commit', 1.0),\n",
       " 'negotiating': ('negotiate', 1.0),\n",
       " 'prompted': ('triggered', 1.0),\n",
       " 'lucrative': ('profitable', 1.0),\n",
       " 'scooter': ('bike', 1.0),\n",
       " 'mars': ('Mars', 1.0),\n",
       " 'medically': ('scientifically', 1.0),\n",
       " 'Austrian': ('Austria', 1.0),\n",
       " 'checklist': ('guidelines', 1.0),\n",
       " 'disabilities': ('disability', 1.0),\n",
       " 'condemned': ('condemn', 1.0),\n",
       " 'vessels': ('vessel', 1.0),\n",
       " 'dangerously': ('dangerous', 1.0),\n",
       " 'ribbon': ('fabric', 1.0),\n",
       " 'cooperate': ('cooperation', 1.0),\n",
       " 'BRAVE': ('Brave', 1.0),\n",
       " 'creeping': ('creeps', 1.0),\n",
       " '83': ('87', 1.0),\n",
       " 'USPS': ('U.S', 1.0),\n",
       " 'nuanced': ('thoughtful', 1.0),\n",
       " 'Yum': ('yummy', 1.0),\n",
       " 'inter': ('II', 1.0),\n",
       " 'gossip': ('celebrity', 1.0),\n",
       " 'Odin': ('Zeus', 1.0),\n",
       " 'grapes': ('fruit', 1.0),\n",
       " 'Lvl': ('lvl', 1.0),\n",
       " 'precautions': ('dangers', 1.0),\n",
       " 'satellites': ('satellite', 1.0),\n",
       " 'Charge': ('charge', 1.0),\n",
       " 'benchmarks': ('objectives', 1.0),\n",
       " 'hinder': ('interfere', 1.0),\n",
       " 'arcane': ('spells', 1.0),\n",
       " 'goddamned': ('Goddamn', 1.0),\n",
       " 'formally': ('officially', 1.0),\n",
       " 'SV': ('SK', 1.0),\n",
       " 'inspect': ('inspection', 1.0),\n",
       " 'Jar': ('jar', 1.0),\n",
       " 'shitstorm': ('clusterfuck', 1.0),\n",
       " 'quieter': ('quiet', 1.0),\n",
       " 'holster': ('pistol', 1.0),\n",
       " 'Sold': ('sold', 1.0),\n",
       " 'unfunny': ('hilariously', 1.0),\n",
       " 'affirmative': ('discrimination', 1.0),\n",
       " 'Na': ('na', 1.0),\n",
       " 'TS': ('TB', 1.0),\n",
       " 'Camera': ('camera', 1.0),\n",
       " 'ND': ('th', 1.0),\n",
       " 'belonged': ('belongs', 1.0),\n",
       " 'berries': ('fruits', 1.0),\n",
       " 'glorified': ('glory', 1.0),\n",
       " 'plumbing': ('repairs', 1.0),\n",
       " 'mechs': ('mech', 1.0),\n",
       " 'Copy': ('copy', 1.0),\n",
       " 'averaged': ('averages', 1.0),\n",
       " 'MADE': ('Made', 1.0),\n",
       " 'openings': ('opening', 1.0),\n",
       " 'punt': ('touchdown', 1.0),\n",
       " 'Kiss': ('kiss', 1.0),\n",
       " '09': ('08', 1.0),\n",
       " 'webcam': ('Cam', 1.0),\n",
       " 'Desert': ('desert', 1.0),\n",
       " 'Wendy': ('Lisa', 1.0),\n",
       " 'convictions': ('conviction', 1.0),\n",
       " 'Visual': ('visual', 1.0),\n",
       " '210': ('220', 1.0),\n",
       " 'Reaper': ('Demon', 1.0),\n",
       " 'Paladin': ('paladin', 1.0),\n",
       " 'prosecute': ('prosecution', 1.0),\n",
       " 'NRA': ('GOP', 1.0),\n",
       " 'heir': ('throne', 1.0),\n",
       " 'Eternal': ('eternal', 1.0),\n",
       " 'heap': ('pile', 1.0),\n",
       " 'Fellow': ('fellow', 1.0),\n",
       " 'inclusion': ('participation', 1.0),\n",
       " 'Bros.': ('Bros', 1.0),\n",
       " 'Humanity': ('humanity', 1.0),\n",
       " 'Nathan': ('Matthew', 1.0),\n",
       " 'aggregate': ('amount', 1.0),\n",
       " 'DAT': ('Dat', 1.0),\n",
       " 'diminishing': ('decreasing', 1.0),\n",
       " 'brushed': ('rubbed', 1.0),\n",
       " 'specialize': ('specialized', 1.0),\n",
       " 'similarity': ('similarities', 1.0),\n",
       " 'Microcenter': ('NCIX', 1.0),\n",
       " 'swamp': ('Jungle', 1.0),\n",
       " 'graphs': ('graph', 1.0),\n",
       " 'Que': ('que', 1.0),\n",
       " 'trench': ('dug', 1.0),\n",
       " 'randomness': ('Random', 1.0),\n",
       " 'obscene': ('outrageous', 1.0),\n",
       " 'G.': ('g', 1.0),\n",
       " 'economical': ('inexpensive', 1.0),\n",
       " 'deprived': ('oppressed', 1.0),\n",
       " 'utilized': ('utilize', 1.0),\n",
       " 'Dumbledore': ('Harry', 1.0),\n",
       " 'praising': ('criticizing', 1.0),\n",
       " 'Marcus': ('Brandon', 1.0),\n",
       " 'apes': ('ape', 1.0),\n",
       " 'Forgetting': ('forgetting', 1.0),\n",
       " 'gaze': ('stare', 1.0),\n",
       " 'neon': ('lights', 1.0),\n",
       " 'slayer': ('Demon', 1.0),\n",
       " 'evasion': ('fraud', 1.0),\n",
       " 'plea': ('conviction', 1.0),\n",
       " 'rests': ('sits', 1.0),\n",
       " 'looting': ('loot', 1.0),\n",
       " 'File': ('file', 1.0),\n",
       " 'fodder': ('crop', 1.0),\n",
       " 'overlay': ('layer', 1.0),\n",
       " 'playful': ('quirky', 1.0),\n",
       " 'Teddy': ('bunny', 1.0),\n",
       " 'barbaric': ('cruel', 1.0),\n",
       " 'espresso': ('Coffee', 1.0),\n",
       " 'WRs': ('QBs', 1.0),\n",
       " 'nifty': ('Neat', 1.0),\n",
       " 'LAST': ('Last', 1.0),\n",
       " 'slips': ('slip', 1.0),\n",
       " 'orbs': ('orb', 1.0),\n",
       " 'scarcity': ('shortage', 1.0),\n",
       " 'confined': ('restricted', 1.0),\n",
       " 'Winston': ('Hamilton', 1.0),\n",
       " 'amd': ('AMD', 1.0),\n",
       " 'robbing': ('stealing', 1.0),\n",
       " 'bangs': ('curly', 1.0),\n",
       " 'halls': ('Hall', 1.0),\n",
       " 'Sooo': ('sooo', 1.0),\n",
       " 'Bolt': ('bolt', 1.0),\n",
       " 'dar': ('este', 1.0),\n",
       " 'Marty': ('Keith', 1.0),\n",
       " 'Europa': ('Madrid', 1.0),\n",
       " 'leans': ('leaning', 1.0),\n",
       " 'bugging': ('bothering', 1.0),\n",
       " 'pearl': ('Pearl', 1.0),\n",
       " 'leftist': ('socialist', 1.0),\n",
       " 'contests': ('contest', 1.0),\n",
       " 'MW2': ('BF3', 1.0),\n",
       " 'MAYBE': ('Maybe', 1.0),\n",
       " 'electron': ('electrons', 1.0),\n",
       " 'philosopher': ('philosophers', 1.0),\n",
       " 'originals': ('Original', 1.0),\n",
       " 'sings': ('singing', 1.0),\n",
       " 'Elsa': ('Leona', 1.0),\n",
       " 'Eevee': ('Pokémon', 1.0),\n",
       " 'Greatest': ('greatest', 1.0),\n",
       " 'airplanes': ('airplane', 1.0),\n",
       " 'brawl': ('Brawl', 1.0),\n",
       " 'Lights': ('lights', 1.0),\n",
       " 'chassis': ('rear', 1.0),\n",
       " 'changer': ('stereo', 1.0),\n",
       " 'pillows': ('pillow', 1.0),\n",
       " 'Gandhi': ('hitler', 1.0),\n",
       " 'clowns': ('clown', 1.0),\n",
       " 'onboard': ('aboard', 1.0),\n",
       " 'chem': ('maths', 1.0),\n",
       " 'T2': ('T1', 1.0),\n",
       " 'undergraduate': ('graduate', 1.0),\n",
       " 'investor': ('investors', 1.0),\n",
       " 'sob': ('Sigh', 1.0),\n",
       " 'recommends': ('recommended', 1.0),\n",
       " 'unimportant': ('insignificant', 1.0),\n",
       " 'NICE': ('Nice', 1.0),\n",
       " 'grins': ('grin', 1.0),\n",
       " 'Dale': ('Jeff', 1.0),\n",
       " 'Malaysia': ('Singapore', 1.0),\n",
       " 'crypto': ('encryption', 1.0),\n",
       " 'Diana': ('Elizabeth', 1.0),\n",
       " 'shortened': ('shorter', 1.0),\n",
       " 'dwarves': ('elves', 1.0),\n",
       " 'Path': ('path', 1.0),\n",
       " 'ignite': ('spark', 1.0),\n",
       " 'dom': ('fetish', 1.0),\n",
       " 'Racism': ('racism', 1.0),\n",
       " 'GONNA': ('Gonna', 1.0),\n",
       " 'recipient': ('received', 1.0),\n",
       " 'PUT': ('Put', 1.0),\n",
       " 'Collection': ('collection', 1.0),\n",
       " 'sobre': ('por', 1.0),\n",
       " 'Survivor': ('survivor', 1.0),\n",
       " 'portrayal': ('portrayed', 1.0),\n",
       " 'WAR': ('War', 1.0),\n",
       " 'reactors': ('reactor', 1.0),\n",
       " 'interstate': ('highways', 1.0),\n",
       " 'Fusion': ('fusion', 1.0),\n",
       " 'pong': ('ping', 1.0),\n",
       " 'gloss': ('matte', 1.0),\n",
       " 'cleavage': ('breasts', 1.0),\n",
       " 'dim': ('lit', 1.0),\n",
       " 'pissy': ('whiny', 1.0),\n",
       " 'GS': ('GT', 1.0),\n",
       " 'Penis': ('penis', 1.0),\n",
       " 'whos': ('Hes', 1.0),\n",
       " 'ported': ('modded', 1.0),\n",
       " 'pardon': ('Forgive', 1.0),\n",
       " 'hr': ('HR', 1.0),\n",
       " 'FM': ('Radio', 1.0),\n",
       " 'fiddle': ('violin', 1.0),\n",
       " 'Shell': ('shell', 1.0),\n",
       " 'Lately': ('lately', 1.0),\n",
       " 'crow': ('owl', 1.0),\n",
       " 'stocked': ('shelves', 1.0),\n",
       " 'prosecutor': ('prosecution', 1.0),\n",
       " 'JavaScript': ('javascript', 1.0),\n",
       " 'unlocks': ('unlock', 1.0),\n",
       " '06': ('07', 1.0),\n",
       " 'Sega': ('nintendo', 1.0),\n",
       " 'biologically': ('biological', 1.0),\n",
       " 'prostate': ('Cancer', 1.0),\n",
       " 'Luther': ('Martin', 1.0),\n",
       " 'dietary': ('diets', 1.0),\n",
       " 'Baker': ('Mason', 1.0),\n",
       " 'dynasty': ('reign', 1.0),\n",
       " 'Crosby': ('Nash', 1.0),\n",
       " 'Weapons': ('weapons', 1.0),\n",
       " 'retaliation': ('harassment', 1.0),\n",
       " 'Belgian': ('Belgium', 1.0),\n",
       " 'resign': ('retire', 1.0),\n",
       " 'fraternity': ('frat', 1.0),\n",
       " 'savage': ('Savage', 1.0),\n",
       " 'flock': ('sheep', 1.0),\n",
       " 'Midnight': ('midnight', 1.0),\n",
       " 'Breakfast': ('breakfast', 1.0),\n",
       " 'metrics': ('methodology', 1.0),\n",
       " 'Vatican': ('pope', 1.0),\n",
       " 'expires': ('expire', 1.0),\n",
       " 'DIE': ('Die', 1.0),\n",
       " 'redstone': ('minecraft', 1.0),\n",
       " 'contemplating': ('Considering', 1.0),\n",
       " 'Jacob': ('Edward', 1.0),\n",
       " 'Mustang': ('Ford', 1.0),\n",
       " 'promotions': ('promotion', 1.0),\n",
       " 'timid': ('Timid', 1.0),\n",
       " 'associates': ('associate', 1.0),\n",
       " 'spout': ('spouting', 1.0),\n",
       " 'canal': ('River', 1.0),\n",
       " 'reflecting': ('reflected', 1.0),\n",
       " 'nova': ('Nova', 1.0),\n",
       " 'flashed': ('flashing', 1.0),\n",
       " 'qb': ('QB', 1.0),\n",
       " 'Czech': ('russian', 1.0),\n",
       " 'Leaf': ('leaf', 1.0),\n",
       " 'VHS': ('DVD', 1.0),\n",
       " 'reiterate': ('clarify', 1.0),\n",
       " 'cooldowns': ('cooldown', 1.0),\n",
       " 'Sunny': ('sunny', 1.0),\n",
       " 'profanity': ('swearing', 1.0),\n",
       " 'guitarist': ('drummer', 1.0),\n",
       " 'compassionate': ('caring', 1.0),\n",
       " 'FINALLY': ('Finally', 1.0),\n",
       " 'trumps': ('dictates', 1.0),\n",
       " 'ticks': ('tick', 1.0),\n",
       " 'recognizable': ('iconic', 1.0),\n",
       " 'Trigger': ('trigger', 1.0),\n",
       " 'ffs': ('Jeez', 1.0),\n",
       " 'whoops': ('Whoops', 1.0),\n",
       " 'planting': ('planted', 1.0),\n",
       " 'affiliation': ('affiliated', 1.0),\n",
       " 'clinics': ('clinic', 1.0),\n",
       " 'summoning': ('summon', 1.0),\n",
       " 'LS': ('CP', 1.0),\n",
       " 'acquisition': ('acquired', 1.0),\n",
       " 'Divine': ('divine', 1.0),\n",
       " 'Ezreal': ('Teemo', 1.0),\n",
       " 'DICE': ('dice', 1.0),\n",
       " 'Mercury': ('toxic', 1.0),\n",
       " 'misogynist': ('misogynistic', 1.0),\n",
       " 'batches': ('batch', 1.0),\n",
       " 'disrupt': ('interfere', 1.0),\n",
       " 'volatile': ('unstable', 1.0),\n",
       " 'lobbyists': ('lobbying', 1.0),\n",
       " 'Install': ('install', 1.0),\n",
       " 'communists': ('Communist', 1.0),\n",
       " 'Lucy': ('Emma', 1.0),\n",
       " 'provoke': ('provoking', 1.0),\n",
       " 'reconcile': ('rationalize', 1.0),\n",
       " 'Pony': ('pony', 1.0),\n",
       " 'Puerto': ('Mexico', 1.0),\n",
       " 'Dodge': ('dodge', 1.0),\n",
       " 'PI': ('Pi', 1.0),\n",
       " 'Critical': ('critical', 1.0),\n",
       " 'E.g.': ('e.g.', 1.0),\n",
       " 'progressively': ('gradually', 1.0),\n",
       " 'stationed': ('troops', 1.0),\n",
       " 'breastfeeding': ('pregnancy', 1.0),\n",
       " 'Downvotes': ('downvotes', 1.0),\n",
       " 'charter': ('sailing', 1.0),\n",
       " 'indoctrination': ('propaganda', 1.0),\n",
       " 'mascot': ('logo', 1.0),\n",
       " 'Runner': ('runner', 1.0),\n",
       " 'Ashe': ('Arthur', 1.0),\n",
       " 'mammals': ('species', 1.0),\n",
       " 'apathetic': ('clueless', 1.0),\n",
       " 'monopolies': ('monopoly', 1.0),\n",
       " 'crutch': ('distraction', 1.0),\n",
       " 'ashes': ('Ash', 1.0),\n",
       " 'moaning': ('moan', 1.0),\n",
       " 'degrading': ('insulting', 1.0),\n",
       " 'Socialism': ('socialism', 1.0),\n",
       " 'deductible': ('expenses', 1.0),\n",
       " 'slamming': ('banging', 1.0),\n",
       " 'Ducks': ('ducks', 1.0),\n",
       " 'Sand': ('sand', 1.0),\n",
       " 'israel': ('Israel', 1.0),\n",
       " 'appointments': ('appointment', 1.0),\n",
       " 'declaration': ('declaring', 1.0),\n",
       " 'Guardians': ('guardian', 1.0),\n",
       " 'RS': ('r', 1.0),\n",
       " 'Wireless': ('wireless', 1.0),\n",
       " 'Cities': ('cities', 1.0),\n",
       " 'condolences': ('sympathy', 1.0),\n",
       " 'camel': ('donkey', 1.0),\n",
       " 'woken': ('woke', 1.0),\n",
       " 'Turner': ('Harris', 1.0),\n",
       " 'Whilst': ('whilst', 1.0),\n",
       " 'с': ('и', 1.0),\n",
       " 'OWS': ('Putin', 1.0),\n",
       " 'covenant': ('salvation', 1.0),\n",
       " 'Bernie': ('Barry', 1.0),\n",
       " 'powerless': ('helpless', 1.0),\n",
       " 'Bama': ('LSU', 1.0),\n",
       " 'purity': ('Pure', 1.0),\n",
       " 'homie': ('nigga', 1.0),\n",
       " 'BJ': ('blowjob', 1.0),\n",
       " 'calf': ('thigh', 1.0),\n",
       " 'laggy': ('framerate', 1.0),\n",
       " 'widget': ('sidebar', 1.0),\n",
       " 'ep': ('EP', 1.0),\n",
       " 'Victor': ('Alexander', 1.0),\n",
       " 'ZERO': ('Zero', 1.0),\n",
       " 'scariest': ('weirdest', 1.0),\n",
       " 'implicit': ('explicit', 1.0),\n",
       " '+5': ('+2', 1.0),\n",
       " 'photographers': ('photographer', 1.0),\n",
       " 'intrusive': ('distracting', 1.0),\n",
       " 'harbor': ('shore', 1.0),\n",
       " 'seating': ('seats', 1.0),\n",
       " 'snapping': ('snapped', 1.0),\n",
       " 'recycled': ('recycling', 1.0),\n",
       " 'devils': ('Devil', 1.0),\n",
       " 'UFO': ('NASA', 1.0),\n",
       " 'atmospheric': ('ambient', 1.0),\n",
       " 'mah': ('yah', 1.0),\n",
       " 'Raven': ('Ravens', 1.0),\n",
       " 'vaccinated': ('vaccine', 1.0),\n",
       " 'chanting': ('chant', 1.0),\n",
       " 'GPUs': ('gpu', 1.0),\n",
       " 'Bud': ('bud', 1.0),\n",
       " 'Schoolboy': ('lad', 1.0),\n",
       " 'buffet': ('breakfast', 1.0),\n",
       " 'Catch': ('catch', 1.0),\n",
       " 'dipping': ('dip', 1.0),\n",
       " 'definitively': ('objectively', 1.0),\n",
       " 'equilibrium': ('equation', 1.0),\n",
       " 'contexts': ('Context', 1.0),\n",
       " 'picturing': ('imagining', 1.0),\n",
       " 'Oak': ('pine', 1.0),\n",
       " 'slider': ('slide', 1.0),\n",
       " 'TF': ('TL', 1.0),\n",
       " 'FUCK.': ('IMO.', 1.0),\n",
       " 'Brit': ('Brits', 1.0),\n",
       " 'shines': ('shining', 1.0),\n",
       " 'debunked': ('refute', 1.0),\n",
       " 'evenings': ('mornings', 1.0),\n",
       " 'bribe': ('convince', 1.0),\n",
       " 'Comparing': ('comparing', 1.0),\n",
       " 'Milwaukee': ('Chicago', 1.0),\n",
       " 'specials': ('discounts', 1.0),\n",
       " 'inhale': ('breathe', 1.0),\n",
       " 'feral': ('Cats', 1.0),\n",
       " 'adjustable': ('adjustment', 1.0),\n",
       " 'airports': ('airport', 1.0),\n",
       " 'sheltered': ('shelter', 1.0),\n",
       " 'imprisoned': ('prisoner', 1.0),\n",
       " 'Ridley': ('turtles', 1.0),\n",
       " 'infer': ('imply', 1.0),\n",
       " 'almond': ('coconut', 1.0),\n",
       " 'rebuilt': ('rebuild', 1.0),\n",
       " 'extraction': ('extract', 1.0),\n",
       " 'begs': ('begging', 1.0),\n",
       " 'peach': ('strawberry', 1.0),\n",
       " 'Honor': ('honor', 1.0),\n",
       " 'prescribe': ('prescribed', 1.0),\n",
       " 'obsessive': ('obsession', 1.0),\n",
       " 'mainland': ('islands', 1.0),\n",
       " 'eleven': ('twelve', 1.0),\n",
       " 'oceans': ('Ocean', 1.0),\n",
       " 'passer': ('quarterback', 1.0),\n",
       " 'Papa': ('mama', 1.0),\n",
       " 'Modular': ('modular', 1.0),\n",
       " 'mailbox': ('inbox', 1.0),\n",
       " 'announcer': ('broadcast', 1.0),\n",
       " 'servants': ('priests', 1.0),\n",
       " 'C4': ('C9', 1.0),\n",
       " 'peed': ('pee', 1.0),\n",
       " '99.9': ('%', 1.0),\n",
       " 'slogan': ('motto', 1.0),\n",
       " 'Negative': ('negative', 1.0),\n",
       " 'Success': ('success', 1.0),\n",
       " 'CG': ('CGI', 1.0),\n",
       " 'uphill': ('downhill', 1.0),\n",
       " 'Supposedly': ('supposedly', 1.0),\n",
       " 'multiplier': ('multiply', 1.0),\n",
       " 'tutor': ('teacher', 1.0),\n",
       " 'Idaho': ('Utah', 1.0),\n",
       " 'Cola': ('Pepsi', 1.0),\n",
       " '270': ('240', 1.0),\n",
       " 'hawk': ('Hawk', 1.0),\n",
       " 'seasoned': ('grilled', 1.0),\n",
       " 'Golf': ('golf', 1.0),\n",
       " 'GoT': ('GOT', 1.0),\n",
       " 'Jeep': ('Ford', 1.0),\n",
       " 'Sona': ('Luna', 1.0),\n",
       " 'Hear': ('hear', 1.0),\n",
       " 'bidding': ('bid', 1.0),\n",
       " 'WoW.': ('LOL.', 1.0),\n",
       " 'diary': ('journal', 1.0),\n",
       " 'incase': ('luckily', 1.0),\n",
       " 'uhh': ('Uhh', 1.0),\n",
       " 'abomination': ('vile', 1.0),\n",
       " 'technician': ('Engineer', 1.0),\n",
       " 'to-day': ('tho', 1.0),\n",
       " 'misplaced': ('mistaken', 1.0),\n",
       " 'aloud': ('loudly', 1.0),\n",
       " 'gasp': ('shudder', 1.0),\n",
       " 'domains': ('domain', 1.0),\n",
       " 'experimentation': ('experiments', 1.0),\n",
       " 'epidemic': ('plague', 1.0),\n",
       " 'Horde': ('horde', 1.0),\n",
       " 'flats': ('flat', 1.0),\n",
       " 'Milan': ('Barcelona', 1.0),\n",
       " 'outbreak': ('flu', 1.0),\n",
       " 'ohm': ('watts', 1.0),\n",
       " 'vanity': ('Mirror', 1.0),\n",
       " 'Surprise': ('surprise', 1.0),\n",
       " 'für': ('und', 1.0),\n",
       " 'Spy': ('spy', 1.0),\n",
       " 'regulars': ('locals', 1.0),\n",
       " 'culprit': ('suspect', 1.0),\n",
       " 'fouls': ('foul', 1.0),\n",
       " 'Mystery': ('mystery', 1.0),\n",
       " 'inane': ('nonsensical', 1.0),\n",
       " 'commentator': ('commentators', 1.0),\n",
       " 'verdict': ('jury', 1.0),\n",
       " 'sinks': ('sink', 1.0),\n",
       " 'allowance': ('expenses', 1.0),\n",
       " 'pimp': ('thug', 1.0),\n",
       " 'rituals': ('ritual', 1.0),\n",
       " '1400': ('1200', 1.0),\n",
       " 'Missed': ('missed', 1.0),\n",
       " 'despicable': ('vile', 1.0),\n",
       " 'parte': ('una', 1.0),\n",
       " 'HOT': ('Hot', 1.0),\n",
       " 'Location': ('location', 1.0),\n",
       " 'Myself': ('myself', 1.0),\n",
       " 'mansion': ('House', 1.0),\n",
       " 'diploma': ('graduate', 1.0),\n",
       " 'CW': ('CB', 1.0),\n",
       " 'informal': ('formal', 1.0),\n",
       " 'Hour': ('hour', 1.0),\n",
       " 'Baron': ('baron', 1.0),\n",
       " 'sickening': ('disgusting', 1.0),\n",
       " 'override': ('specify', 1.0),\n",
       " 'rotting': ('rotten', 1.0),\n",
       " 'll': ('youll', 1.0),\n",
       " 'Toy': ('toy', 1.0),\n",
       " 'cooled': ('cooling', 1.0),\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.vocab.Vocab at 0x132fda4c048>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ci(komyagin):\n",
    "    txt = (nltk.Text(komyagin))\n",
    "    return(ConcordanceIndex(txt))\n",
    "\n",
    "#for some reason having difficulty subsetting by tokens instead of characters:: instead just use enough \n",
    "#characters to later be able to reliably get a 5 word radius\n",
    "\n",
    "def get_context(ci, word, width=150, lines=100):\n",
    "    \n",
    "    half_width =  (width - len(word) - 2) // 2\n",
    "    context = width // 4 # approx number of words of context\n",
    "    num = 5\n",
    "    results = []\n",
    "    offsets = ci.offsets(word)\n",
    "    \n",
    "    if offsets:\n",
    "        for i in offsets:\n",
    "            query_word = ci._tokens[i]\n",
    "  \n",
    "            left_context = ci._tokens[max(0, i - context) : i]\n",
    "            right_context = ci._tokens[i + 1 : i + context]\n",
    "           \n",
    "            left_print = \" \".join(left_context)[-half_width:]\n",
    "            right_print = \" \".join(right_context)[:half_width]\n",
    "                \n",
    "            full_line_print = \" \".join([left_print, query_word, right_print])\n",
    "            \n",
    "            results.append(full_line_print)\n",
    "            \n",
    "    return ([num, results])\n",
    "\n",
    "def clean_text(txt_ls):\n",
    "    \n",
    "    translator = str.maketrans('','', sub('\\#', '', punctuation))\n",
    "\n",
    "    clean_txt_ls = []\n",
    "    for i in txt_ls:\n",
    "        n = i.split()\n",
    "        str_ = \"\"\n",
    "        for z in n:\n",
    "            z = z.lower()\n",
    "            s = z.translate(str.maketrans(translator))\n",
    "            if s not in stopwords:\n",
    "          #  print(s)\n",
    "                str_ = str_ + \" \" + s\n",
    "        clean_txt_ls.append(str_[1:])\n",
    "        \n",
    "    return(clean_txt_ls)\n",
    "\n",
    "def clean_context(ci, target_word1, target_word2, window = 5):\n",
    "    tox = []\n",
    "    #gets context around a target word\n",
    "    word_one_context = get_context(ci, target_word1)\n",
    "   \n",
    "     #takes this context and reshapes it into a -5 to + 5 window\n",
    "    for i in word_one_context[1]:\n",
    "        split_i = i.split()\n",
    "        for z in split_i:\n",
    "            tox.append(z)\n",
    "            \n",
    "    to_mend = list(enumerate(tox))\n",
    "    \n",
    "    mended_tox = []\n",
    "    #modifications necessary to account for words at the start and end of corpus\n",
    "    for z in to_mend:\n",
    "        if z[1] == target_word1:\n",
    "            if z[0] < (window):\n",
    "                padded = tox[0:(z[0]+window)]\n",
    "                mended_tox.append(padded)\n",
    "            elif z[0] > (len(tox) - 1):\n",
    "                padded = tox[(z[0] -window):(len(tox)-1)]\n",
    "                mended_tox.append(padded)\n",
    "            else: \n",
    "                padded = tox[(z[0] -window):(z[0] + window)]\n",
    "                mended_tox.append(padded)\n",
    "#reshaping list into format that can be more quickly processed    \n",
    "    final_tox = []\n",
    "    for i in mended_tox:\n",
    "        for n in i:\n",
    "            final_tox.append(n)\n",
    "    return(final_tox)\n",
    "\n",
    "\n",
    "#gathers all contexts of the word in results\n",
    "def mutual_informativity(ci, target_word1, target_word2, total_count, window = 10):\n",
    "   \n",
    "    final_tox =  clean_context(ci, target_word1, target_word2, window = 10)\n",
    "    \n",
    "#this will return the count of target_word2 in the vicinity of target_word1\n",
    "    prob_numx = Counter(final_tox)\n",
    "    prob_num = prob_numx[target_word2]\n",
    "    P_rc = prob_num/total_count\n",
    "    P_r = (count(ci, target_word1))/total_count\n",
    "    P_c = (count(ci, target_word2))/total_count\n",
    " #checking for indexing errors and overlap errors.\n",
    "#fix to overlap is a bit of a hack fix, most notable problems are double counting \n",
    "#so if number of cooccurances is greater than number of times a word appears in \n",
    "#a document, then it replaces cooccurance with the total number of times\n",
    "#except in the case that this is an odd number (since it's not being double-counted then,\n",
    "#it's being double counted once and has another appearance)\n",
    "#odd numbers, is P_c - 1/total. \n",
    "#this still may cause some errors\n",
    "    if P_rc == 0:\n",
    "     #   print(prob_numx)\n",
    "        return ([target_word1, target_word2, \"ERROR\"])\n",
    "    elif P_rc > P_c:\n",
    "     #   print(target_word2)\n",
    "        rounded = int(P_rc/P_c)\n",
    "        nr = P_rc/P_c\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c - 1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    elif P_rc > P_r:\n",
    "        rounded = int(P_rc/P_r)\n",
    "        nr = P_rc/P_r\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c -1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    else:\n",
    "        mutinf = math.log10(P_rc/(P_r*P_c))\n",
    "    return ([target_word1, target_word2, mutinf])\n",
    "\n",
    "def count(ci, word):\n",
    "   \n",
    "    offsets = ci.offsets(word)\n",
    "\n",
    "    return len(offsets)\n",
    "\n",
    "def pmi(text):\n",
    "    '''\n",
    "    iterates through and finds shit\n",
    "    '''\n",
    "    \n",
    "    clean_doc = clean_text(text.split())\n",
    "    total_count = len(clean_doc)\n",
    "    \n",
    "    test_ci = make_ci(clean_doc)\n",
    "    \n",
    "    pmi_list = []\n",
    "    ordered_set_hold = []\n",
    "    ordered_set = [i for i in clean_doc if i not in ordered_set_hold and len(i) > 0]\n",
    "\n",
    "    \n",
    "    index = 0\n",
    "    #realistically only words that at some point occur in a 3 word window are really worth looking at esp with these short texts\n",
    "    for i in enumerate(ordered_set):\n",
    "        if i[0] < (len((ordered_set))-1):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 1], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        if i[0] < (len(ordered_set)-2):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 2], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)        \n",
    "        if i[0] < (len(ordered_set)-3):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 3], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        else:\n",
    "            return(pmi_list)\n",
    "        \n",
    "#just to allow for sorting by actual pmi index\n",
    "def takeSecond(elem):\n",
    "    #print(elem[2])\n",
    "    return elem[2]\n",
    "\n",
    "def pmi_high(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = True)\n",
    "    return cat[:n]\n",
    "\n",
    "def pmi_low(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = False)\n",
    "    return cat[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_words(text):\n",
    "    '''\n",
    "    iterates through and finds shit\n",
    "    '''\n",
    "    \n",
    "    clean_doc = clean_text(text.split())\n",
    "    \n",
    "    words_pairs = []\n",
    "\n",
    "    index = 0\n",
    "    #realistically only words that at some point occur in a 3 word window are really worth looking at esp with these short texts\n",
    "    for i in enumerate(clean_doc):\n",
    "        if i[0] < (len((clean_doc))-1):\n",
    "            item = (i[1], clean_doc[i[0] + 1])\n",
    "            if(item not in words_pairs) & ((clean_doc[i[0] + 1], i[1]) not in word_pairs) & (item[0] != item[1]):\n",
    "                words_pairs.append(item)\n",
    "        if i[0] < (len(clean_doc)-2):\n",
    "            item =  (i[1], clean_doc[i[0] + 2])\n",
    "            if(item not in words_pairs) & ((clean_doc[i[0] + 2], i[1]) not in word_pairs) & (item[0] != item[1]):\n",
    "                words_pairs.append(item)        \n",
    "        if i[0] < (len(clean_doc)-3):\n",
    "            item =  (i[1], ordered_set[i[0] + 3])\n",
    "            if(item not in words_pairs) & ((clean_doc[i[0] + 3], i[1]) not in word_pairs) & (item[0] != item[1]):\n",
    "                words_pairs.append(item)\n",
    "        else:\n",
    "            return(words_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madke\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  del sys.path[0]\n",
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "pmi_test = pmi(test_doc)\n",
    "bar = progressbar.ProgressBar(maxval=4890, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "\n",
    "word_pairs = []\n",
    "item_dis = []\n",
    "latent_dis = []\n",
    "bar.start()\n",
    "x = 0\n",
    "for i in pmi_high(pmi_test, 4890):\n",
    "    if i != None:\n",
    "        j, k , l = latent_meaning_spacy([i[0], i[1]])\n",
    "        word_pairs.append(j)\n",
    "        item_dis.append(k)\n",
    "        latent_dis.append(l)\n",
    "        bar.update(x + 1)\n",
    "        x = x + 1\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(test_doc, str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_meanings = pd.DataFrame({\n",
    "    \n",
    "    'word_pairs': word_pairs,\n",
    "    'item_dis': item_dis,\n",
    "    'latent_dis': latent_dis\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_meanings['difference']= latent_meanings['item_dis'] - latent_meanings['latent_dis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pairs</th>\n",
       "      <th>item_dis</th>\n",
       "      <th>latent_dis</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>[whale, sperm]</td>\n",
       "      <td>0.406892</td>\n",
       "      <td>0.258778</td>\n",
       "      <td>0.148114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>[sperm, whale]</td>\n",
       "      <td>0.406892</td>\n",
       "      <td>0.258778</td>\n",
       "      <td>0.148114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>[new, york]</td>\n",
       "      <td>0.386062</td>\n",
       "      <td>0.240859</td>\n",
       "      <td>0.145203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>[pikes, wallers]</td>\n",
       "      <td>0.336901</td>\n",
       "      <td>0.213824</td>\n",
       "      <td>0.123077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>[swiftness, letter]</td>\n",
       "      <td>0.173467</td>\n",
       "      <td>0.054868</td>\n",
       "      <td>0.118599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4423</th>\n",
       "      <td>[whales, spouting]</td>\n",
       "      <td>0.271814</td>\n",
       "      <td>0.157196</td>\n",
       "      <td>0.114618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>[prey, swallow]</td>\n",
       "      <td>0.436203</td>\n",
       "      <td>0.325043</td>\n",
       "      <td>0.111159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>[sperm, whales]</td>\n",
       "      <td>0.357355</td>\n",
       "      <td>0.251056</td>\n",
       "      <td>0.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>[time, ay]</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>-0.016900</td>\n",
       "      <td>0.104742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>[manner, mischievous]</td>\n",
       "      <td>0.391926</td>\n",
       "      <td>0.302750</td>\n",
       "      <td>0.089176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>[let, fly]</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.328514</td>\n",
       "      <td>0.089068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>[fly, let]</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.328514</td>\n",
       "      <td>0.089068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>[swiftness, utterly]</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.199226</td>\n",
       "      <td>0.085875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>[first, love]</td>\n",
       "      <td>0.385973</td>\n",
       "      <td>0.301596</td>\n",
       "      <td>0.084377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4666</th>\n",
       "      <td>[iceland, whale]</td>\n",
       "      <td>0.286496</td>\n",
       "      <td>0.204363</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2181</th>\n",
       "      <td>[greenland, whale]</td>\n",
       "      <td>0.286496</td>\n",
       "      <td>0.204363</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>[whale, greenland]</td>\n",
       "      <td>0.286496</td>\n",
       "      <td>0.204363</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>[ceti, whale]</td>\n",
       "      <td>0.104671</td>\n",
       "      <td>0.032224</td>\n",
       "      <td>0.072448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>[shore, maine]</td>\n",
       "      <td>0.362042</td>\n",
       "      <td>0.289880</td>\n",
       "      <td>0.072162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>[pacific, ocean]</td>\n",
       "      <td>0.546190</td>\n",
       "      <td>0.477253</td>\n",
       "      <td>0.068937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>[annals, great]</td>\n",
       "      <td>0.451441</td>\n",
       "      <td>0.385773</td>\n",
       "      <td>0.065668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>[man, swiftness]</td>\n",
       "      <td>0.221315</td>\n",
       "      <td>0.156597</td>\n",
       "      <td>0.064719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>[captain, lee]</td>\n",
       "      <td>0.274487</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.062887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>[mouth, boat]</td>\n",
       "      <td>0.315955</td>\n",
       "      <td>0.253699</td>\n",
       "      <td>0.062256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>[fire, water]</td>\n",
       "      <td>0.452047</td>\n",
       "      <td>0.390114</td>\n",
       "      <td>0.061934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>[art, mote]</td>\n",
       "      <td>0.203319</td>\n",
       "      <td>0.142176</td>\n",
       "      <td>0.061143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4723</th>\n",
       "      <td>[shipwreck, whale]</td>\n",
       "      <td>0.398374</td>\n",
       "      <td>0.339649</td>\n",
       "      <td>0.058726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>[whale, shipwreck]</td>\n",
       "      <td>0.398374</td>\n",
       "      <td>0.339649</td>\n",
       "      <td>0.058726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>[obed, macys]</td>\n",
       "      <td>0.134113</td>\n",
       "      <td>0.075438</td>\n",
       "      <td>0.058675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4826</th>\n",
       "      <td>[sperm, ocean]</td>\n",
       "      <td>0.275812</td>\n",
       "      <td>0.217350</td>\n",
       "      <td>0.058462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>[object, whale]</td>\n",
       "      <td>0.147978</td>\n",
       "      <td>0.089876</td>\n",
       "      <td>0.058102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>[sing, ay]</td>\n",
       "      <td>0.218170</td>\n",
       "      <td>0.160612</td>\n",
       "      <td>0.057559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>[daniel, websters]</td>\n",
       "      <td>0.201784</td>\n",
       "      <td>0.144966</td>\n",
       "      <td>0.056818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>[blood, gushing]</td>\n",
       "      <td>0.376060</td>\n",
       "      <td>0.319507</td>\n",
       "      <td>0.056553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>[swiftness, sometimes]</td>\n",
       "      <td>0.248296</td>\n",
       "      <td>0.192369</td>\n",
       "      <td>0.055928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>[obey, e]</td>\n",
       "      <td>0.127889</td>\n",
       "      <td>0.073705</td>\n",
       "      <td>0.054184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>[replied, samuel]</td>\n",
       "      <td>0.163081</td>\n",
       "      <td>0.109163</td>\n",
       "      <td>0.053919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>[fishes, speak]</td>\n",
       "      <td>0.198780</td>\n",
       "      <td>0.144965</td>\n",
       "      <td>0.053815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>[first, mate]</td>\n",
       "      <td>0.290444</td>\n",
       "      <td>0.236708</td>\n",
       "      <td>0.053736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>[fishes, make]</td>\n",
       "      <td>0.338389</td>\n",
       "      <td>0.284970</td>\n",
       "      <td>0.053420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>[von, letters]</td>\n",
       "      <td>0.082967</td>\n",
       "      <td>0.030284</td>\n",
       "      <td>0.052683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>[play, fishes]</td>\n",
       "      <td>0.228844</td>\n",
       "      <td>0.176717</td>\n",
       "      <td>0.052127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>[round, october]</td>\n",
       "      <td>0.249890</td>\n",
       "      <td>0.198230</td>\n",
       "      <td>0.051660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>[king, ad]</td>\n",
       "      <td>0.170697</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>0.050867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3557</th>\n",
       "      <td>[see, ay]</td>\n",
       "      <td>0.062823</td>\n",
       "      <td>0.011977</td>\n",
       "      <td>0.050845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>[whales, breaches]</td>\n",
       "      <td>0.154602</td>\n",
       "      <td>0.104173</td>\n",
       "      <td>0.050428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word_pairs  item_dis  latent_dis  difference\n",
       "4752          [whale, sperm]  0.406892    0.258778    0.148114\n",
       "3619          [sperm, whale]  0.406892    0.258778    0.148114\n",
       "2112             [new, york]  0.386062    0.240859    0.145203\n",
       "292         [pikes, wallers]  0.336901    0.213824    0.123077\n",
       "1818     [swiftness, letter]  0.173467    0.054868    0.118599\n",
       "4423      [whales, spouting]  0.271814    0.157196    0.114618\n",
       "2532         [prey, swallow]  0.436203    0.325043    0.111159\n",
       "4887         [sperm, whales]  0.357355    0.251056    0.106300\n",
       "2210              [time, ay]  0.087841   -0.016900    0.104742\n",
       "1080   [manner, mischievous]  0.391926    0.302750    0.089176\n",
       "2453              [let, fly]  0.417582    0.328514    0.089068\n",
       "1613              [fly, let]  0.417582    0.328514    0.089068\n",
       "2158    [swiftness, utterly]  0.285100    0.199226    0.085875\n",
       "3049           [first, love]  0.385973    0.301596    0.084377\n",
       "4666        [iceland, whale]  0.286496    0.204363    0.082132\n",
       "2181      [greenland, whale]  0.286496    0.204363    0.082132\n",
       "4756      [whale, greenland]  0.286496    0.204363    0.082132\n",
       "1660           [ceti, whale]  0.104671    0.032224    0.072448\n",
       "2477          [shore, maine]  0.362042    0.289880    0.072162\n",
       "3884        [pacific, ocean]  0.546190    0.477253    0.068937\n",
       "4136         [annals, great]  0.451441    0.385773    0.065668\n",
       "2964        [man, swiftness]  0.221315    0.156597    0.064719\n",
       "2202          [captain, lee]  0.274487    0.211600    0.062887\n",
       "4358           [mouth, boat]  0.315955    0.253699    0.062256\n",
       "1954           [fire, water]  0.452047    0.390114    0.061934\n",
       "1648             [art, mote]  0.203319    0.142176    0.061143\n",
       "4723      [shipwreck, whale]  0.398374    0.339649    0.058726\n",
       "4721      [whale, shipwreck]  0.398374    0.339649    0.058726\n",
       "875            [obed, macys]  0.134113    0.075438    0.058675\n",
       "4826          [sperm, ocean]  0.275812    0.217350    0.058462\n",
       "4781         [object, whale]  0.147978    0.089876    0.058102\n",
       "2771              [sing, ay]  0.218170    0.160612    0.057559\n",
       "1221      [daniel, websters]  0.201784    0.144966    0.056818\n",
       "1997        [blood, gushing]  0.376060    0.319507    0.056553\n",
       "3091  [swiftness, sometimes]  0.248296    0.192369    0.055928\n",
       "1820               [obey, e]  0.127889    0.073705    0.054184\n",
       "1277       [replied, samuel]  0.163081    0.109163    0.053919\n",
       "2611         [fishes, speak]  0.198780    0.144965    0.053815\n",
       "3730           [first, mate]  0.290444    0.236708    0.053736\n",
       "3466          [fishes, make]  0.338389    0.284970    0.053420\n",
       "588           [von, letters]  0.082967    0.030284    0.052683\n",
       "3507          [play, fishes]  0.228844    0.176717    0.052127\n",
       "2760        [round, october]  0.249890    0.198230    0.051660\n",
       "4477              [king, ad]  0.170697    0.119831    0.050867\n",
       "3557               [see, ay]  0.062823    0.011977    0.050845\n",
       "4439      [whales, breaches]  0.154602    0.104173    0.050428"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_meanings[(latent_meanings['difference'] > .05) & (latent_meanings['item_dis'] > 0.05)].sort_values(by = 'difference', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_lexicals(input_value, n = 200):\n",
    "    if isinstance(input_value, str):\n",
    "        pmi_text = pmi(input_value)\n",
    "    else:\n",
    "        return('error')\n",
    "    word_pairs = []\n",
    "    item_dis = []\n",
    "    latent_dis = []\n",
    "    x = 0\n",
    "    for i in pmi_high(pmi_text, n):\n",
    "        if i != None:\n",
    "            j, k , l = latent_meaning_spacy([i[0], i[1]])\n",
    "            word_pairs.append(j)\n",
    "            item_dis.append(k)\n",
    "            latent_dis.append(l)\n",
    "            #bar update\n",
    "            \n",
    "    latent_meanings = pd.DataFrame({\n",
    "    \n",
    "    'word_pairs': word_pairs,\n",
    "    'item_dis': item_dis,\n",
    "    'latent_dis': latent_dis\n",
    "    \n",
    "    })\n",
    "    latent_meanings['difference']= latent_meanings['item_dis'] - latent_meanings['latent_dis']\n",
    "    lexicals = latent_meanings[(latent_meanings['difference'] > .05) & (latent_meanings['item_dis'] > 0.05)].sort_values(by = 'difference', ascending = False)\n",
    "    lexicals_temp = [str(i) for i in lexicals['word_pairs']]\n",
    "    if len(lexicals_temp) > 0:\n",
    "        return( '_'.join(lexicals_temp))\n",
    "    else:\n",
    "        return('no relevant pairs in this selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madke\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lexicals(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
