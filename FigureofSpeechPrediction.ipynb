{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk import ConcordanceIndex\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "vocab = nlp.vocab.strings\n",
    "\n",
    "\n",
    "from string import punctuation\n",
    "from re import sub\n",
    "punctuation = punctuation +'”'+'“'+'’' + '—' + '’' + '‘' +'0123456789'\n",
    "\n",
    "import progressbar\n",
    "\n",
    "test_doc = open(\"./test_doc.txt\", \"r\", encoding = \"utf-8\")\n",
    "test_doc = test_doc.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['whom','hast','thou','therein', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, topn=10):\n",
    "    #https://stackoverflow.com/questions/57697374/list-most-similar-words-in-spacy-in-pretrained-model\n",
    "    ms = nlp.vocab.vectors.most_similar(nlp(word).vector.reshape(1,nlp(word).vector.shape[0]), n=topn)\n",
    "    words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "    distances = ms[2]\n",
    "    return words, distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def latent_meaning_spacy(i, top_ = 10):\n",
    "    '''\n",
    "    INPUT: word tuple, topn\n",
    "    \n",
    "    OUTPUT: word tuple, the distance between the two original words, and the distance between the topn related words\n",
    "    '''\n",
    "    if(i[0] in vocab) & (i[1] in vocab):\n",
    "        \n",
    "        first_close, first_close_distances = most_similar(i[0], topn= top_)\n",
    "        second_close, second_close_distances = most_similar(i[1], topn= top_)\n",
    "        first_vec = nlp.vocab[i[0]].vector\n",
    "        second_vec = nlp.vocab[i[1]].vector\n",
    "        item_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + nlp.vocab[z].vector\n",
    "        \n",
    "        for z in second_close:\n",
    "            second_vec = second_vec +  nlp.vocab[z].vector\n",
    "        \n",
    "        first_vec = first_vec - nlp.vocab[i[0]].vector\n",
    "        second_vec = second_vec - nlp.vocab[i[1]].vector\n",
    "        \n",
    "        latent_dis = np.dot(first_vec, second_vec)/(np.linalg.norm(first_vec)*np.linalg.norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, latent_dis])\n",
    "    else:\n",
    "        return([None, None, None])\n",
    "\n",
    "\n",
    "def latent_meaning(i, model3):\n",
    "    \n",
    "    if(i[0] in model3.wv.vocab) & (i[1] in model3.wv.vocab):\n",
    "        first_close = list(model3.wv.most_similar(i[0], topn= 5))\n",
    "        second_close = list(model3.wv.most_similar(i[1], topn= 5))\n",
    "        \n",
    "        first_vec = model3.wv.get_vector(i[0])\n",
    "        second_vec = model3.wv.get_vector(i[1])\n",
    "        \n",
    "        item_dis = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        for z in first_close:\n",
    "            first_vec = first_vec + model3.wv.get_vector(z[0])\n",
    "        for n in second_close:\n",
    "            second_vec = second_vec + model3.wv.get_vector(n[0])\n",
    "        \n",
    "        first_vec = first_vec - model3[i[0]]\n",
    "        second_vec = second_vec - model3[i[1]]\n",
    "        cos_sim = dot(first_vec, second_vec)/(norm(first_vec)*norm(second_vec))\n",
    "        \n",
    "        return([i, item_dis, cos_sim])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PMI is defined as //pmi(r,c)=log(P(r,c)/(P(r)*P(c)))//, with P(r,c) being the\n",
    "#probability of co-occurrence and P(r) and P(c) the probability of\n",
    "#occurrence of two words (estimated via frequency)\n",
    "\n",
    "#- I considered words as co-occurring if they occurred within a window\n",
    "#of 5 words:\n",
    "#no no yes yes yes yes target yes yes yes yes no no\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ci(komyagin):\n",
    "    txt = (nltk.Text(komyagin))\n",
    "    return(ConcordanceIndex(txt))\n",
    "\n",
    "#for some reason having difficulty subsetting by tokens instead of characters:: instead just use enough \n",
    "#characters to later be able to reliably get a 5 word radius\n",
    "\n",
    "def get_context(ci, word, width=150, lines=100):\n",
    "    \n",
    "    half_width =  (width - len(word) - 2) // 2\n",
    "    context = width // 4 # approx number of words of context\n",
    "    num = 5\n",
    "    results = []\n",
    "    offsets = ci.offsets(word)\n",
    "    \n",
    "    if offsets:\n",
    "        for i in offsets:\n",
    "            query_word = ci._tokens[i]\n",
    "  \n",
    "            left_context = ci._tokens[max(0, i - context) : i]\n",
    "            right_context = ci._tokens[i + 1 : i + context]\n",
    "           \n",
    "            left_print = \" \".join(left_context)[-half_width:]\n",
    "            right_print = \" \".join(right_context)[:half_width]\n",
    "                \n",
    "            full_line_print = \" \".join([left_print, query_word, right_print])\n",
    "            \n",
    "            results.append(full_line_print)\n",
    "            \n",
    "    return ([num, results])\n",
    "\n",
    "def clean_text(txt_ls):\n",
    "    \n",
    "    translator = str.maketrans('','', sub('\\#', '', punctuation))\n",
    "\n",
    "    clean_txt_ls = []\n",
    "    for i in txt_ls:\n",
    "        n = i.split()\n",
    "        str_ = \"\"\n",
    "        for z in n:\n",
    "            z = z.lower()\n",
    "            s = z.translate(str.maketrans(translator))\n",
    "            if s not in stopwords:\n",
    "          #  print(s)\n",
    "                str_ = str_ + \" \" + s\n",
    "        clean_txt_ls.append(str_[1:])\n",
    "        \n",
    "    return(clean_txt_ls)\n",
    "\n",
    "def clean_context(ci, target_word1, target_word2, window = 5):\n",
    "    tox = []\n",
    "    #gets context around a target word\n",
    "    word_one_context = get_context(ci, target_word1)\n",
    "   \n",
    "     #takes this context and reshapes it into a -5 to + 5 window\n",
    "    for i in word_one_context[1]:\n",
    "        split_i = i.split()\n",
    "        for z in split_i:\n",
    "            tox.append(z)\n",
    "            \n",
    "    to_mend = list(enumerate(tox))\n",
    "    \n",
    "    mended_tox = []\n",
    "    #modifications necessary to account for words at the start and end of corpus\n",
    "    for z in to_mend:\n",
    "        if z[1] == target_word1:\n",
    "            if z[0] < (window):\n",
    "                padded = tox[0:(z[0]+window)]\n",
    "                mended_tox.append(padded)\n",
    "            elif z[0] > (len(tox) - 1):\n",
    "                padded = tox[(z[0] -window):(len(tox)-1)]\n",
    "                mended_tox.append(padded)\n",
    "            else: \n",
    "                padded = tox[(z[0] -window):(z[0] + window)]\n",
    "                mended_tox.append(padded)\n",
    "#reshaping list into format that can be more quickly processed    \n",
    "    final_tox = []\n",
    "    for i in mended_tox:\n",
    "        for n in i:\n",
    "            final_tox.append(n)\n",
    "    return(final_tox)\n",
    "\n",
    "\n",
    "#gathers all contexts of the word in results\n",
    "def mutual_informativity(ci, target_word1, target_word2, total_count, window = 10):\n",
    "   \n",
    "    final_tox =  clean_context(ci, target_word1, target_word2, window = 10)\n",
    "    \n",
    "#this will return the count of target_word2 in the vicinity of target_word1\n",
    "    prob_numx = Counter(final_tox)\n",
    "    prob_num = prob_numx[target_word2]\n",
    "    P_rc = prob_num/total_count\n",
    "    P_r = (count(ci, target_word1))/total_count\n",
    "    P_c = (count(ci, target_word2))/total_count\n",
    " #checking for indexing errors and overlap errors.\n",
    "#fix to overlap is a bit of a hack fix, most notable problems are double counting \n",
    "#so if number of cooccurances is greater than number of times a word appears in \n",
    "#a document, then it replaces cooccurance with the total number of times\n",
    "#except in the case that this is an odd number (since it's not being double-counted then,\n",
    "#it's being double counted once and has another appearance)\n",
    "#odd numbers, is P_c - 1/total. \n",
    "#this still may cause some errors\n",
    "    if P_rc == 0:\n",
    "     #   print(prob_numx)\n",
    "        return ([target_word1, target_word2, \"ERROR\"])\n",
    "    elif P_rc > P_c:\n",
    "     #   print(target_word2)\n",
    "        rounded = int(P_rc/P_c)\n",
    "        nr = P_rc/P_c\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c - 1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    elif P_rc > P_r:\n",
    "        rounded = int(P_rc/P_r)\n",
    "        nr = P_rc/P_r\n",
    "        if (rounded > nr):\n",
    "            P_n = P_c -1/total_count\n",
    "        else:\n",
    "            P_n = P_c\n",
    "        mutinf = math.log10(P_n/(P_r*P_c))\n",
    "        #print([P_rc, P_c, P_r, P_n])\n",
    "    else:\n",
    "        mutinf = math.log10(P_rc/(P_r*P_c))\n",
    "    return ([target_word1, target_word2, mutinf])\n",
    "\n",
    "def count(ci, word):\n",
    "   \n",
    "    offsets = ci.offsets(word)\n",
    "\n",
    "    return len(offsets)\n",
    "\n",
    "def pmi(text):\n",
    "    '''\n",
    "    iterates through and finds shit\n",
    "    '''\n",
    "    \n",
    "    clean_doc = clean_text(text.split())\n",
    "    total_count = len(clean_doc)\n",
    "    \n",
    "    test_ci = make_ci(clean_doc)\n",
    "    \n",
    "    pmi_list = []\n",
    "    ordered_set_hold = []\n",
    "    ordered_set = [i for i in clean_doc if i not in ordered_set_hold and len(i) > 0]\n",
    "\n",
    "    \n",
    "    index = 0\n",
    "    #realistically only words that at some point occur in a 3 word window are really worth looking at esp with these short texts\n",
    "    for i in enumerate(ordered_set):\n",
    "        if i[0] < (len((ordered_set))-1):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 1], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        if i[0] < (len(ordered_set)-2):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 2], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)        \n",
    "        if i[0] < (len(ordered_set)-3):\n",
    "            item = mutual_informativity(test_ci, i[1], ordered_set[i[0] + 3], total_count)\n",
    "            if(item not in pmi_list) & (item[0] != item[1]):\n",
    "                pmi_list.append(item)\n",
    "        else:\n",
    "            return(pmi_list)\n",
    "        \n",
    "#just to allow for sorting by actual pmi index\n",
    "def takeSecond(elem):\n",
    "    #print(elem[2])\n",
    "    return elem[2]\n",
    "\n",
    "def pmi_high(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = True)\n",
    "    return cat[:n]\n",
    "\n",
    "def pmi_low(pmi_output, n):\n",
    "    cat = pmi_output\n",
    "    cat.sort(key = takeSecond, reverse = False)\n",
    "    return cat[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pair_words(text):\n",
    "    '''\n",
    "    iterates through and finds shit\n",
    "    '''\n",
    "    \n",
    "    clean_doc = clean_text(text.split())\n",
    "    total_count = len(clean_doc)\n",
    "    \n",
    "    test_ci = make_ci(clean_doc)\n",
    "    \n",
    "    words_pairs = []\n",
    "    ordered_set_hold = []\n",
    "    ordered_set = [i for i in clean_doc if i not in ordered_set_hold and len(i) > 0]\n",
    "\n",
    "    \n",
    "    index = 0\n",
    "    #realistically only words that at some point occur in a 3 word window are really worth looking at esp with these short texts\n",
    "    for i in enumerate(ordered_set):\n",
    "        if i[0] < (len((ordered_set))-1):\n",
    "            item = (i[1], ordered_set[i[0] + 1])\n",
    "            if(item not in words_pairs) & (item[0] != item[1]):\n",
    "                words_pairs.append(item)\n",
    "        if i[0] < (len(ordered_set)-2):\n",
    "            item =  (i[1], ordered_set[i[0] + 2])\n",
    "            if(item not in words_pairs) & (item[0] != item[1]):\n",
    "                words_pairs.append(item)        \n",
    "        if i[0] < (len(ordered_set)-3):\n",
    "            item =  (i[1], ordered_set[i[0] + 3])\n",
    "            if(item not in words_pairs) & (item[0] != item[1]):\n",
    "                words_pairs.append(item)\n",
    "        else:\n",
    "            return(words_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madke\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  del sys.path[0]\n",
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "pmi_test = pmi(test_doc)\n",
    "bar = progressbar.ProgressBar(maxval=4890, \\\n",
    "    widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "\n",
    "word_pairs = []\n",
    "item_dis = []\n",
    "latent_dis = []\n",
    "bar.start()\n",
    "x = 0\n",
    "for i in pmi_high(pmi_test, 4890):\n",
    "    if i != None:\n",
    "        j, k , l = latent_meaning_spacy([i[0], i[1]])\n",
    "        word_pairs.append(j)\n",
    "        item_dis.append(k)\n",
    "        latent_dis.append(l)\n",
    "        bar.update(x + 1)\n",
    "        x = x + 1\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(test_doc, str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_meanings = pd.DataFrame({\n",
    "    \n",
    "    'word_pairs': word_pairs,\n",
    "    'item_dis': item_dis,\n",
    "    'latent_dis': latent_dis\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_meanings['difference']= latent_meanings['item_dis'] - latent_meanings['latent_dis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_pairs</th>\n",
       "      <th>item_dis</th>\n",
       "      <th>latent_dis</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>[whale, sperm]</td>\n",
       "      <td>0.406892</td>\n",
       "      <td>0.258778</td>\n",
       "      <td>0.148114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>[sperm, whale]</td>\n",
       "      <td>0.406892</td>\n",
       "      <td>0.258778</td>\n",
       "      <td>0.148114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2112</th>\n",
       "      <td>[new, york]</td>\n",
       "      <td>0.386062</td>\n",
       "      <td>0.240859</td>\n",
       "      <td>0.145203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>[pikes, wallers]</td>\n",
       "      <td>0.336901</td>\n",
       "      <td>0.213824</td>\n",
       "      <td>0.123077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>[swiftness, letter]</td>\n",
       "      <td>0.173467</td>\n",
       "      <td>0.054868</td>\n",
       "      <td>0.118599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4423</th>\n",
       "      <td>[whales, spouting]</td>\n",
       "      <td>0.271814</td>\n",
       "      <td>0.157196</td>\n",
       "      <td>0.114618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>[prey, swallow]</td>\n",
       "      <td>0.436203</td>\n",
       "      <td>0.325043</td>\n",
       "      <td>0.111159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>[sperm, whales]</td>\n",
       "      <td>0.357355</td>\n",
       "      <td>0.251056</td>\n",
       "      <td>0.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>[time, ay]</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>-0.016900</td>\n",
       "      <td>0.104742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>[manner, mischievous]</td>\n",
       "      <td>0.391926</td>\n",
       "      <td>0.302750</td>\n",
       "      <td>0.089176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>[let, fly]</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.328514</td>\n",
       "      <td>0.089068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>[fly, let]</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>0.328514</td>\n",
       "      <td>0.089068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>[swiftness, utterly]</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.199226</td>\n",
       "      <td>0.085875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>[first, love]</td>\n",
       "      <td>0.385973</td>\n",
       "      <td>0.301596</td>\n",
       "      <td>0.084377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4666</th>\n",
       "      <td>[iceland, whale]</td>\n",
       "      <td>0.286496</td>\n",
       "      <td>0.204363</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2181</th>\n",
       "      <td>[greenland, whale]</td>\n",
       "      <td>0.286496</td>\n",
       "      <td>0.204363</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>[whale, greenland]</td>\n",
       "      <td>0.286496</td>\n",
       "      <td>0.204363</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>[ceti, whale]</td>\n",
       "      <td>0.104671</td>\n",
       "      <td>0.032224</td>\n",
       "      <td>0.072448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>[shore, maine]</td>\n",
       "      <td>0.362042</td>\n",
       "      <td>0.289880</td>\n",
       "      <td>0.072162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3884</th>\n",
       "      <td>[pacific, ocean]</td>\n",
       "      <td>0.546190</td>\n",
       "      <td>0.477253</td>\n",
       "      <td>0.068937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>[annals, great]</td>\n",
       "      <td>0.451441</td>\n",
       "      <td>0.385773</td>\n",
       "      <td>0.065668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2964</th>\n",
       "      <td>[man, swiftness]</td>\n",
       "      <td>0.221315</td>\n",
       "      <td>0.156597</td>\n",
       "      <td>0.064719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>[captain, lee]</td>\n",
       "      <td>0.274487</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.062887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4358</th>\n",
       "      <td>[mouth, boat]</td>\n",
       "      <td>0.315955</td>\n",
       "      <td>0.253699</td>\n",
       "      <td>0.062256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>[fire, water]</td>\n",
       "      <td>0.452047</td>\n",
       "      <td>0.390114</td>\n",
       "      <td>0.061934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>[art, mote]</td>\n",
       "      <td>0.203319</td>\n",
       "      <td>0.142176</td>\n",
       "      <td>0.061143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4723</th>\n",
       "      <td>[shipwreck, whale]</td>\n",
       "      <td>0.398374</td>\n",
       "      <td>0.339649</td>\n",
       "      <td>0.058726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4721</th>\n",
       "      <td>[whale, shipwreck]</td>\n",
       "      <td>0.398374</td>\n",
       "      <td>0.339649</td>\n",
       "      <td>0.058726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>[obed, macys]</td>\n",
       "      <td>0.134113</td>\n",
       "      <td>0.075438</td>\n",
       "      <td>0.058675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4826</th>\n",
       "      <td>[sperm, ocean]</td>\n",
       "      <td>0.275812</td>\n",
       "      <td>0.217350</td>\n",
       "      <td>0.058462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>[object, whale]</td>\n",
       "      <td>0.147978</td>\n",
       "      <td>0.089876</td>\n",
       "      <td>0.058102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>[sing, ay]</td>\n",
       "      <td>0.218170</td>\n",
       "      <td>0.160612</td>\n",
       "      <td>0.057559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>[daniel, websters]</td>\n",
       "      <td>0.201784</td>\n",
       "      <td>0.144966</td>\n",
       "      <td>0.056818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>[blood, gushing]</td>\n",
       "      <td>0.376060</td>\n",
       "      <td>0.319507</td>\n",
       "      <td>0.056553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>[swiftness, sometimes]</td>\n",
       "      <td>0.248296</td>\n",
       "      <td>0.192369</td>\n",
       "      <td>0.055928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>[obey, e]</td>\n",
       "      <td>0.127889</td>\n",
       "      <td>0.073705</td>\n",
       "      <td>0.054184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>[replied, samuel]</td>\n",
       "      <td>0.163081</td>\n",
       "      <td>0.109163</td>\n",
       "      <td>0.053919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>[fishes, speak]</td>\n",
       "      <td>0.198780</td>\n",
       "      <td>0.144965</td>\n",
       "      <td>0.053815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>[first, mate]</td>\n",
       "      <td>0.290444</td>\n",
       "      <td>0.236708</td>\n",
       "      <td>0.053736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>[fishes, make]</td>\n",
       "      <td>0.338389</td>\n",
       "      <td>0.284970</td>\n",
       "      <td>0.053420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>[von, letters]</td>\n",
       "      <td>0.082967</td>\n",
       "      <td>0.030284</td>\n",
       "      <td>0.052683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>[play, fishes]</td>\n",
       "      <td>0.228844</td>\n",
       "      <td>0.176717</td>\n",
       "      <td>0.052127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2760</th>\n",
       "      <td>[round, october]</td>\n",
       "      <td>0.249890</td>\n",
       "      <td>0.198230</td>\n",
       "      <td>0.051660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>[king, ad]</td>\n",
       "      <td>0.170697</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>0.050867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3557</th>\n",
       "      <td>[see, ay]</td>\n",
       "      <td>0.062823</td>\n",
       "      <td>0.011977</td>\n",
       "      <td>0.050845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>[whales, breaches]</td>\n",
       "      <td>0.154602</td>\n",
       "      <td>0.104173</td>\n",
       "      <td>0.050428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word_pairs  item_dis  latent_dis  difference\n",
       "4752          [whale, sperm]  0.406892    0.258778    0.148114\n",
       "3619          [sperm, whale]  0.406892    0.258778    0.148114\n",
       "2112             [new, york]  0.386062    0.240859    0.145203\n",
       "292         [pikes, wallers]  0.336901    0.213824    0.123077\n",
       "1818     [swiftness, letter]  0.173467    0.054868    0.118599\n",
       "4423      [whales, spouting]  0.271814    0.157196    0.114618\n",
       "2532         [prey, swallow]  0.436203    0.325043    0.111159\n",
       "4887         [sperm, whales]  0.357355    0.251056    0.106300\n",
       "2210              [time, ay]  0.087841   -0.016900    0.104742\n",
       "1080   [manner, mischievous]  0.391926    0.302750    0.089176\n",
       "2453              [let, fly]  0.417582    0.328514    0.089068\n",
       "1613              [fly, let]  0.417582    0.328514    0.089068\n",
       "2158    [swiftness, utterly]  0.285100    0.199226    0.085875\n",
       "3049           [first, love]  0.385973    0.301596    0.084377\n",
       "4666        [iceland, whale]  0.286496    0.204363    0.082132\n",
       "2181      [greenland, whale]  0.286496    0.204363    0.082132\n",
       "4756      [whale, greenland]  0.286496    0.204363    0.082132\n",
       "1660           [ceti, whale]  0.104671    0.032224    0.072448\n",
       "2477          [shore, maine]  0.362042    0.289880    0.072162\n",
       "3884        [pacific, ocean]  0.546190    0.477253    0.068937\n",
       "4136         [annals, great]  0.451441    0.385773    0.065668\n",
       "2964        [man, swiftness]  0.221315    0.156597    0.064719\n",
       "2202          [captain, lee]  0.274487    0.211600    0.062887\n",
       "4358           [mouth, boat]  0.315955    0.253699    0.062256\n",
       "1954           [fire, water]  0.452047    0.390114    0.061934\n",
       "1648             [art, mote]  0.203319    0.142176    0.061143\n",
       "4723      [shipwreck, whale]  0.398374    0.339649    0.058726\n",
       "4721      [whale, shipwreck]  0.398374    0.339649    0.058726\n",
       "875            [obed, macys]  0.134113    0.075438    0.058675\n",
       "4826          [sperm, ocean]  0.275812    0.217350    0.058462\n",
       "4781         [object, whale]  0.147978    0.089876    0.058102\n",
       "2771              [sing, ay]  0.218170    0.160612    0.057559\n",
       "1221      [daniel, websters]  0.201784    0.144966    0.056818\n",
       "1997        [blood, gushing]  0.376060    0.319507    0.056553\n",
       "3091  [swiftness, sometimes]  0.248296    0.192369    0.055928\n",
       "1820               [obey, e]  0.127889    0.073705    0.054184\n",
       "1277       [replied, samuel]  0.163081    0.109163    0.053919\n",
       "2611         [fishes, speak]  0.198780    0.144965    0.053815\n",
       "3730           [first, mate]  0.290444    0.236708    0.053736\n",
       "3466          [fishes, make]  0.338389    0.284970    0.053420\n",
       "588           [von, letters]  0.082967    0.030284    0.052683\n",
       "3507          [play, fishes]  0.228844    0.176717    0.052127\n",
       "2760        [round, october]  0.249890    0.198230    0.051660\n",
       "4477              [king, ad]  0.170697    0.119831    0.050867\n",
       "3557               [see, ay]  0.062823    0.011977    0.050845\n",
       "4439      [whales, breaches]  0.154602    0.104173    0.050428"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_meanings[(latent_meanings['difference'] > .05) & (latent_meanings['item_dis'] > 0.05)].sort_values(by = 'difference', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_lexicals(input_value, n = 200):\n",
    "    if isinstance(input_value, str):\n",
    "        pmi_text = pmi(input_value)\n",
    "    else:\n",
    "        return('error')\n",
    "    word_pairs = []\n",
    "    item_dis = []\n",
    "    latent_dis = []\n",
    "    x = 0\n",
    "    for i in pmi_high(pmi_text, n):\n",
    "        if i != None:\n",
    "            j, k , l = latent_meaning_spacy([i[0], i[1]])\n",
    "            word_pairs.append(j)\n",
    "            item_dis.append(k)\n",
    "            latent_dis.append(l)\n",
    "            #bar update\n",
    "            \n",
    "    latent_meanings = pd.DataFrame({\n",
    "    \n",
    "    'word_pairs': word_pairs,\n",
    "    'item_dis': item_dis,\n",
    "    'latent_dis': latent_dis\n",
    "    \n",
    "    })\n",
    "    latent_meanings['difference']= latent_meanings['item_dis'] - latent_meanings['latent_dis']\n",
    "    lexicals = latent_meanings[(latent_meanings['difference'] > .05) & (latent_meanings['item_dis'] > 0.05)].sort_values(by = 'difference', ascending = False)\n",
    "    lexicals_temp = [str(i) for i in lexicals['word_pairs']]\n",
    "    if len(lexicals_temp) > 0:\n",
    "        return( '_'.join(lexicals_temp))\n",
    "    else:\n",
    "        return('no relevant pairs in this selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\madke\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_lexicals(test_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
